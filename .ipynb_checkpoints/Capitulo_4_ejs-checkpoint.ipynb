{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Capitulo 4: Spark SQL and DataFrames: Introduction to Built-in Data Sources\n",
    "\n",
    "Iremos leyendo y realizando los ejemplos del capitulo 4 del libro, complementandolo con unos actividades sobre los mismos ejemplos. \n",
    "\n",
    "##### Siempre debemos iniciar una instancia SparkSession al principio. \n",
    "\n",
    "Por lo que antes de comenzar crearemos la SparkSession correspondiente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Creamos SparkSession (IMPORTANTE cambiar el nombre si estamos en otros scripts)\n",
    "spark = SparkSession\\\n",
    "    .builder\\\n",
    "    .appName(\"LibroSpark_cap4\")\\\n",
    "    .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este capitulo y el siguiente exploraremos como funciona la interfaz SparkSQL con alguno de sus componentes externos. Gracias a SparkSQL:\n",
    "- Podemos usar las APIS estructuradas de alto y bajo nivel vistas anteriormente (RDD, DF y DS)\n",
    "- Podemos leer y escribir datos en diferentes formatos (JSON, Tablas Hive, Parquet, Avro, etc)\n",
    "- Podemos hacer consultas mediante conectores JDBC/OBDC de fuente externa con aplicaciones de Business Inteligence como Tableau, PowerBI, RDBMSs como MySQL o PostgreSQL, etc\n",
    "- Tenemos una interfaz de programacion para interactuar con datos estructurados almacenados como tablas o vistas en una bbdd de la Spark Aplication\n",
    "- Podemos interactuar con una shell en SQL para hacer querys, soportando el lenguaje ANSI SQL:2003 y HQL\n",
    "\n",
    "<center><img src=\"./images/SparkSQL_stack.PNG\"></center>\n",
    "\n",
    "Recordemos que para usar SparkSQL en la Spark Aplication utilizamos la SparkSession, que hace como punto de entrada para programar con Spark con el uso de APIS. Para usar querys SQL, utilizamos el metodo sql() sobre la instancia de la SparkSession (spark, ver la SparkSession arriba)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Query Examples\n",
    "\n",
    "Para esta seccion utilizaremos el dataset departuredelays.csv que contiene datos sobre vuelos de US como fecha, retraso, distancia, origen y destino. Leeremos primero el DF y lo guardaremos como vista temporal para poder ejecutar querys en SQL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- date: integer (nullable = true)\n",
      " |-- delay: integer (nullable = true)\n",
      " |-- distance: integer (nullable = true)\n",
      " |-- origin: string (nullable = true)\n",
      " |-- destination: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Cargamos el dataframe departuredelays.csv infiriendo su schema\n",
    "df = (spark.read.format(\"csv\")\n",
    ".option(\"inferSchema\", \"true\")\n",
    ".option(\"header\", \"true\")\n",
    ".load(\"./Datasets/departuredelays.csv\"))\n",
    "\n",
    "# Comprobamos schema\n",
    "df.printSchema()\n",
    "\n",
    "# Si no estuviera bien inferido, se puede crear a través de lenguaje DDL y cambiando el inferSchema por este schema\n",
    "schema = \"`date` STRING, `delay` INT, `distance` INT, `origin` STRING, `destination` STRING\"\n",
    "\n",
    "# Creamos vista temporal de DF cargado con createOrReplaceTempView\n",
    "df.createOrReplaceTempView(\"us_delay_flights_tbl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ya tenemos nuestra vista temporal a través del DF cargado de un archivo csv. Podemos usar sentencias en lenguaje SQL tal y como haríamos con MySQL o PostgreeSQL con el método spark.sql(sentencia). Algunos ejemplos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+-----------+\n",
      "|distance|origin|destination|\n",
      "+--------+------+-----------+\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "+--------+------+-----------+\n",
      "only showing top 10 rows\n",
      "\n",
      "+-------+-----+------+-----------+\n",
      "|   date|delay|origin|destination|\n",
      "+-------+-----+------+-----------+\n",
      "|2190925| 1638|   SFO|        ORD|\n",
      "|1031755|  396|   SFO|        ORD|\n",
      "|1022330|  326|   SFO|        ORD|\n",
      "|1051205|  320|   SFO|        ORD|\n",
      "|1190925|  297|   SFO|        ORD|\n",
      "|2171115|  296|   SFO|        ORD|\n",
      "|1071040|  279|   SFO|        ORD|\n",
      "|1051550|  274|   SFO|        ORD|\n",
      "|3120730|  266|   SFO|        ORD|\n",
      "|1261104|  258|   SFO|        ORD|\n",
      "+-------+-----+------+-----------+\n",
      "only showing top 10 rows\n",
      "\n",
      "+-------------------+\n",
      "|               Date|\n",
      "+-------------------+\n",
      "|1970-01-12 17:54:05|\n",
      "|1970-01-12 20:30:00|\n",
      "|1970-01-12 20:40:45|\n",
      "|1970-01-12 20:30:05|\n",
      "|1970-01-12 23:27:25|\n",
      "|1970-01-12 23:16:45|\n",
      "|1970-01-13 02:14:03|\n",
      "|1970-01-13 02:03:25|\n",
      "|1970-01-13 05:00:45|\n",
      "|1970-01-13 04:50:05|\n",
      "+-------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import *\n",
    "\n",
    "# Utilizamos el metodo el método spark.sql(sentencia)\n",
    "# Importante utilizar las triples comillas \"\"\"\"\"\" para poder ordenar en varias lineas nuestra consulta\n",
    "# Ejemplos de algunas consultas sencillas\n",
    "\n",
    "# 1\n",
    "spark.sql(\"\"\"SELECT distance, origin, destination\n",
    "             FROM us_delay_flights_tbl \n",
    "             WHERE distance > 1000\n",
    "             ORDER BY distance DESC\"\"\").show(10)\n",
    "# 2\n",
    "spark.sql(\"\"\"SELECT date, delay, origin, destination\n",
    "             FROM us_delay_flights_tbl\n",
    "             WHERE delay > 120 AND ORIGIN = 'SFO' AND DESTINATION = 'ORD'\n",
    "             ORDER by delay DESC\"\"\").show(10)\n",
    "\n",
    "# Ejercicio adicional:\n",
    "# convierta la columna de fechas en un formato legible (con CAST)\n",
    "spark.sql(\"\"\"SELECT CAST(date as timestamp) as Date\n",
    "             FROM us_delay_flights_tbl \"\"\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+-----------+-------------+\n",
      "|delay|origin|destination|Flight_Delays|\n",
      "+-----+------+-----------+-------------+\n",
      "|  333|   ABE|        ATL|  Long Delays|\n",
      "|  305|   ABE|        ATL|  Long Delays|\n",
      "|  275|   ABE|        ATL|  Long Delays|\n",
      "|  257|   ABE|        ATL|  Long Delays|\n",
      "|  247|   ABE|        DTW|  Long Delays|\n",
      "|  247|   ABE|        ATL|  Long Delays|\n",
      "|  219|   ABE|        ORD|  Long Delays|\n",
      "|  211|   ABE|        ATL|  Long Delays|\n",
      "|  197|   ABE|        DTW|  Long Delays|\n",
      "|  192|   ABE|        ORD|  Long Delays|\n",
      "+-----+------+-----------+-------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 3 Mostramos un ejemplo de una consulta más completa utilizando la clausula CASE\n",
    "spark.sql(\"\"\"SELECT delay, origin, destination,\n",
    "             CASE\n",
    "                 WHEN delay > 360 THEN 'Very Long Delays'\n",
    "                 WHEN delay > 120 AND delay < 360 THEN 'Long Delays'\n",
    "                 WHEN delay > 60 AND delay < 120 THEN 'Short Delays'\n",
    "                 WHEN delay > 0 and delay < 60 THEN 'Tolerable Delays'\n",
    "                 WHEN delay = 0 THEN 'No Delays'\n",
    "                 ELSE 'Early'\n",
    "             END AS Flight_Delays\n",
    "             FROM us_delay_flights_tbl\n",
    "             ORDER BY origin, delay DESC\"\"\").show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como hemos comentado varias veces, estas consultas de SQLpuro escritas con la interfaz SparkSQL pueden ser escritas de manera equivalente en la API de Dataframe con identico resultado y rendimiento gracias al CatalystOptimizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+-----------+\n",
      "|distance|origin|destination|\n",
      "+--------+------+-----------+\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "+--------+------+-----------+\n",
      "only showing top 10 rows\n",
      "\n",
      "+-------+--------+------+-----------+\n",
      "|   date|distance|origin|destination|\n",
      "+-------+--------+------+-----------+\n",
      "|3201100|    1604|   SFO|        ORD|\n",
      "|3311810|    1604|   SFO|        ORD|\n",
      "|3311405|    1604|   SFO|        ORD|\n",
      "|3120929|    1604|   SFO|        ORD|\n",
      "|3141657|    1604|   SFO|        ORD|\n",
      "|3171251|    1604|   SFO|        ORD|\n",
      "|3171215|    1604|   SFO|        ORD|\n",
      "|3260828|    1604|   SFO|        ORD|\n",
      "|3261106|    1604|   SFO|        ORD|\n",
      "|3272225|    1604|   SFO|        ORD|\n",
      "+-------+--------+------+-----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import *\n",
    "\n",
    "# 1\n",
    "(df.select(\"distance\", \"origin\", \"destination\")\n",
    "   .where(\"distance > 1000\")\n",
    "   .orderBy(\"distance\", ascending=False)\n",
    "   .show(10))\n",
    "\n",
    "# 2\n",
    "(df.select(\"date\", \"distance\", \"origin\", \"destination\")\n",
    "   .where( (df.delay > 120) & (df.origin == 'SFO') & (df.destination == 'ORD') )\n",
    "   .orderBy(\"distance\", ascending=False)\n",
    "   .show(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SQL Tables and Views\n",
    "\n",
    "En esta parte veremos como se crean las tablas y vistas y como las gestiona Spark, tanto en memoria como en disco. Las tablas contienen datos y asociada a cada tabla estan los metadatos, que es la información que describen el contenido de los datos de la tabla como el shcema, descripción, nombre de la tabla, particiones, localización, etc. Los metadatos se almacenan en un metastore central.\n",
    "\n",
    "En lugar de tener un metastore separado para las tablas de Spark, Spark utiliza por defecto el metastore de Apache Hive, ubicado en **/user/hive/warehouse**, para guardar todos los metadatos sobre sus tablas. Puedes uedes cambiar la ubicación por defecto estableciendo la variable de configuración de Spark **spark.sql.warehouse.dir** a otra ubicación, pdudiendose estableceser en local o de manera externa.\n",
    "\n",
    "\n",
    "### Managed vs Unmanaged Tables Differences\n",
    "\n",
    "Puedes crea dos tipos de tablas: **Managed o dirigidas y Unmanaged o no dirigidas.**\n",
    "\n",
    "Para las tablas dirigidas, Spark maneja los metadatos y los datos en el file store, que puede ser en Local, HDFS o otros stores como AmazonS3 o Azure Blob. Para el caso de las no dirigidas Spark solo gestiona los metadatos y tu los gestionas en una fuente de datos externa como Cassandra.\n",
    "\n",
    "En una tabla dirigida, sentencias SQL como DROP TABLE borran los datos y los metadatos mientras que en una tabla no dirigida el mismo comando elimina solo los metadatos y no los datos. \n",
    "\n",
    "\n",
    "## Creating SQL Databases and Tables\n",
    "\n",
    "Por defecto, spark crea las tablas en la bbdd default. Para crear una bbdd podemos usar el lenguaje SQL, en el siguiente ejemplo creamos la bbdd learn_spark_db y la marcamos para que la siguientes tablas que creemos se almacenen en dicha bbdd.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creamos bbdd learn_spark_db y la marcamos\n",
    "spark.sql(\"CREATE DATABASE learn_spark_db\")\n",
    "spark.sql(\"USE learn_spark_db\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a managed table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "Hive support is required to CREATE Hive TABLE (AS SELECT);;\n'CreateTable `managed_us_delay_flights_tbl`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Ignore\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-897ce347a655>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Creamos la tabla con SQL\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m spark.sql(\"\"\"CREATE TABLE IF NOT EXISTS managed_us_delay_flights_tbl (\n\u001b[0m\u001b[0;32m      4\u001b[0m           date STRING, delay INT,distance INT, origin STRING, destination STRING )\"\"\")\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pyspark\\sql\\session.py\u001b[0m in \u001b[0;36msql\u001b[1;34m(self, sqlQuery)\u001b[0m\n\u001b[0;32m    647\u001b[0m         \u001b[1;33m[\u001b[0m\u001b[0mRow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34mu'row1'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34mu'row2'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34mu'row3'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    648\u001b[0m         \"\"\"\n\u001b[1;32m--> 649\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jsparkSession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msql\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msqlQuery\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_wrapped\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    650\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    651\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0msince\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2.0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1302\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1304\u001b[1;33m         return_value = get_return_value(\n\u001b[0m\u001b[0;32m   1305\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0;32m   1306\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    132\u001b[0m                 \u001b[1;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    133\u001b[0m                 \u001b[1;31m# JVM exception message.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 134\u001b[1;33m                 \u001b[0mraise_from\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconverted\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    135\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    136\u001b[0m                 \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mraise_from\u001b[1;34m(e)\u001b[0m\n",
      "\u001b[1;31mAnalysisException\u001b[0m: Hive support is required to CREATE Hive TABLE (AS SELECT);;\n'CreateTable `managed_us_delay_flights_tbl`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Ignore\n"
     ]
    }
   ],
   "source": [
    "# Creamos la tabla con SQL\n",
    "\n",
    "spark.sql(\"\"\"CREATE TABLE IF NOT EXISTS managed_us_delay_flights_tbl (\n",
    "          date STRING, delay INT,distance INT, origin STRING, destination STRING )\"\"\")\n",
    "\n",
    "# Lanza una excepción que indica que se cree la tabla como CTA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "Hive support is required to CREATE Hive TABLE (AS SELECT);;\n'CreateTable `managed_us_delay_flights_tbl`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Ignore\n+- Project [date#16, delay#17, distance#18, origin#19, destination#20]\n   +- SubqueryAlias us_delay_flights_tbl\n      +- Relation[date#16,delay#17,distance#18,origin#19,destination#20] csv\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-25eef304d5ce>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m spark.sql( \"\"\"CREATE TABLE IF NOT EXISTS managed_us_delay_flights_tbl\n\u001b[0m\u001b[0;32m      2\u001b[0m               AS SELECT * FROM us_delay_flights_tbl\"\"\")\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# Lanza otra excepción\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pyspark\\sql\\session.py\u001b[0m in \u001b[0;36msql\u001b[1;34m(self, sqlQuery)\u001b[0m\n\u001b[0;32m    647\u001b[0m         \u001b[1;33m[\u001b[0m\u001b[0mRow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34mu'row1'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34mu'row2'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34mu'row3'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    648\u001b[0m         \"\"\"\n\u001b[1;32m--> 649\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jsparkSession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msql\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msqlQuery\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_wrapped\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    650\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    651\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0msince\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2.0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1302\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1304\u001b[1;33m         return_value = get_return_value(\n\u001b[0m\u001b[0;32m   1305\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0;32m   1306\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    132\u001b[0m                 \u001b[1;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    133\u001b[0m                 \u001b[1;31m# JVM exception message.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 134\u001b[1;33m                 \u001b[0mraise_from\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconverted\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    135\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    136\u001b[0m                 \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mraise_from\u001b[1;34m(e)\u001b[0m\n",
      "\u001b[1;31mAnalysisException\u001b[0m: Hive support is required to CREATE Hive TABLE (AS SELECT);;\n'CreateTable `managed_us_delay_flights_tbl`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Ignore\n+- Project [date#16, delay#17, distance#18, origin#19, destination#20]\n   +- SubqueryAlias us_delay_flights_tbl\n      +- Relation[date#16,delay#17,distance#18,origin#19,destination#20] csv\n"
     ]
    }
   ],
   "source": [
    "spark.sql( \"\"\"CREATE TABLE IF NOT EXISTS managed_us_delay_flights_tbl\n",
    "              AS SELECT * FROM us_delay_flights_tbl\"\"\")\n",
    "\n",
    "# Lanza otra excepción"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creamos la tabla con DF api, cargando primero el DF flights_df del df original y guardandolo con write.saveAsTable\n",
    "flights_df = df\n",
    "flights_df.write.saveAsTable(\"managed_us_delay_flights_tbl_apidf\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a unmanaged table \n",
    "\n",
    "Puedes crear tablas no dirigidas en diferentes formatos como csv, parquet o json accesibles en el file store local de la Spark Aplication. \n",
    "\n",
    "En el siguiente ejemplo creamos una tabla unmanaged de un archivo csv, se pueden ver las dos maneras, SQLpuro y API DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# SQL puro\n",
    "spark.sql(\"\"\" CREATE TABLE us_delay_flights_tbl(date STRING, delay INT, distance INT, origin STRING, destination STRING)\n",
    "              USING csv OPTIONS (PATH'./Datasets/departuredelays.csv') \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DF api (modificamos nombre a us_delay_flights_tbl2 para que no lance error)\n",
    "(df.write\n",
    "   .option(\"path\", \"./Datasets/departuredelays.csv\")\n",
    "   .saveAsTable(\"us_delay_flights_tbl2\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Views\n",
    "\n",
    "Además, en spark se pueden crear vistas de tablas ya existentes. Las vistas pueden ser:\n",
    "- Globales (visibles en todas las SparkSessions de un determinado cluster)\n",
    "- Temporales (desaparecen cuando después de que la aplicación Spark termine)\n",
    "\n",
    "La sintaxis para la creación de vistas es similar a la de la creación de tablas. La diferencia es entre una vista y una tabla es que la en la **vista los datos de la tabla o se guaran despues de que la spark aplication termine.**\n",
    "\n",
    "La **diferencia entre las vistas temporales y globales temporales** es que una vista temporal está ligada a una única SparkSession dentro de una aplicación Spark. En cambio, una vista temporal global es visible en múltiples SparkSessions dentro de una aplicación Spark. \n",
    "\n",
    "Puede crear múltiples SparkSessions dentro de una sola aplicación Spark. Esto puede ser útil en los casos en los que quieras acceder (y combinar) datos de dos diferentes SparkSessions que no comparten las mismas configuraciones de metastore de Hive.\n",
    "\n",
    "Puedes crrear vistas de una tabla SQL para por ejemplo trabajar con submuestras de una tabla y no preocuparte en crear, guardar la tabla. Ejemplos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# En lenguaje SQL puro (no hace falta ejecutae)\n",
    "spark.sql( \"\"\"CREATE OR REPLACE GLOBAL TEMP VIEW us_origin_airport_SFO_global_tmp_view \n",
    "              AS SELECT date, delay, origin, destination from us_delay_flights_tbl \n",
    "              WHERE origin = 'SFO' \"\"\")\n",
    "\n",
    "spark.sql( \"\"\"CREATE OR REPLACE TEMP VIEW us_origin_airport_JFK_tmp_view \n",
    "              AS SELECT date, delay, origin, destination from us_delay_flights_tbl \n",
    "              WHERE origin = 'JFK'\"\"\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# En API dF\n",
    "df_sfo = spark.sql(\"SELECT date, delay, origin, destination FROM us_delay_flights_tbl WHERE origin = 'SFO'\")\n",
    "df_jfk = spark.sql(\"SELECT date, delay, origin, destination FROM us_delay_flights_tbl WHERE origin = 'JFK'\")\n",
    "                   \n",
    "# Creamos vista temporal global (createOrReplaceGlobalTempView) y vista temporal (createOrReplaceTempView)\n",
    "df_sfo.createOrReplaceGlobalTempView(\"us_origin_airport_SFO_global_tmp_view\")\n",
    "df_jfk.createOrReplaceTempView(\"us_origin_airport_JFK_tmp_view\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez creadas las vistas, puedes hacer querys sobre ellas como si de una tabla se tratase, por lo que es mas comodo si no quieres que se guarden los datos despues de la tabla despues de que la spark aplication termine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+------+-----------+\n",
      "|   date|delay|origin|destination|\n",
      "+-------+-----+------+-----------+\n",
      "|1010900|   14|   JFK|        LAX|\n",
      "|1011200|   -3|   JFK|        LAX|\n",
      "|1011900|    2|   JFK|        LAX|\n",
      "|1011700|   11|   JFK|        LAS|\n",
      "|1010800|   -1|   JFK|        SFO|\n",
      "+-------+-----+------+-----------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+-------+-----+------+-----------+\n",
      "|   date|delay|origin|destination|\n",
      "+-------+-----+------+-----------+\n",
      "|1010900|   14|   JFK|        LAX|\n",
      "|1011200|   -3|   JFK|        LAX|\n",
      "|1011900|    2|   JFK|        LAX|\n",
      "|1011700|   11|   JFK|        LAS|\n",
      "|1010800|   -1|   JFK|        SFO|\n",
      "+-------+-----+------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# En SQL (meter entre spark.sql para lanzarlo)\n",
    "spark.sql( \"\"\"SELECT * FROM us_origin_airport_JFK_tmp_view\"\"\" ).show(5)\n",
    "\n",
    "# Python\n",
    "spark.read.table(\"us_origin_airport_JFK_tmp_view\").show(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tambien puedes borrar la vista, ejemplo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# En SQL (meter entre spark.sql para lanzarlo)\n",
    " # DROP VIEW IF EXISTS us_origin_airport_SFO_global_tmp_view;\n",
    " # DROP VIEW IF EXISTS us_origin_airport_JFK_tmp_view\n",
    "\n",
    "# API DF\n",
    "spark.catalog.dropGlobalTempView(\"us_origin_airport_SFO_global_tmp_view\")\n",
    "spark.catalog.dropTempView(\"us_origin_airport_JFK_tmp_view\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Viewing the Metadata\n",
    "\n",
    "Para manejar los metadatos de cada tabla (managed o unmanaged) utilizamos el Catalog, una abtración de SparkSQL para el almacenamiento de los metadatos. \n",
    "\n",
    "Por ejemplo, en una Spark Aplication, despues de crear la spark = SparkSession puedes acceder al almacen de metadatos con estos metodos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Database(name='default', description='default database', locationUri='file:/C:/Users/franciscomanuel.medi/OneDrive%20-%20Bosonit/Escritorio/Practicas/Practica%20Spark/Notebooks/Libro%20Learning%20Spark%20L20/spark-warehouse'),\n",
       " Database(name='learn_spark_db', description='', locationUri='file:/C:/Users/franciscomanuel.medi/OneDrive%20-%20Bosonit/Escritorio/Practicas/Practica%20Spark/Notebooks/Libro%20Learning%20Spark%20L20/spark-warehouse/learn_spark_db.db')]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# metodo listdatabases de catalog\n",
    "spark.catalog.listDatabases()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Table(name='us_delay_flights_tbl', database='learn_spark_db', description=None, tableType='EXTERNAL', isTemporary=False),\n",
       " Table(name='us_delay_flights_tbl', database=None, description=None, tableType='TEMPORARY', isTemporary=True)]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# metodo listTables de catalog\n",
    "spark.catalog.listTables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Column(name='date', description=None, dataType='string', nullable=True, isPartition=False, isBucket=False),\n",
       " Column(name='delay', description=None, dataType='int', nullable=True, isPartition=False, isBucket=False),\n",
       " Column(name='distance', description=None, dataType='int', nullable=True, isPartition=False, isBucket=False),\n",
       " Column(name='origin', description=None, dataType='string', nullable=True, isPartition=False, isBucket=False),\n",
       " Column(name='destination', description=None, dataType='string', nullable=True, isPartition=False, isBucket=False)]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# metodo listTables de catalog\n",
    "spark.catalog.listColumns(\"us_delay_flights_tbl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Caching SQL Tables\n",
    "\n",
    "Puedes cachear y descachear las tablas de SQL y vistas. Puedes especificar una tabla como LAZY, siginifcando esto que se sólo debe almacenarse en la caché cuando se utiliza por primera vez en lugar de inmediatamente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# En SQL (meter entre spark.sql para lanzarlo)\n",
    " # CACHE [LAZY] TABLE <table-name>\n",
    " # UNCACHE TABLE <table-name>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading Tables into DataFrames\n",
    "\n",
    "Hemos visto anteriormente que SparkSQL tiene una amplia variedad a la hora de guardar o leer tanto tablas como DF en diferentes formatos. A su vez, Spark distingue entre:\n",
    "- DataFrameReader para leer datos en diferentes formatos\n",
    "- DataFrameWritter para guardar datos en diferentes formatos\n",
    "\n",
    "### DataFrameReader\n",
    "https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrameReader\n",
    "\n",
    "Podemos leer datos, ya sean DF, archivos, o tablas en base a diferentes formatos, para ver los argumentos del DataFrameReader en el link adjunto de la documentación de spark.\n",
    "\n",
    "Hemos utilizado el DFreader cada vez que hemos creado un DF a través de un archivo, ya sea CSV, Parquet, Json, etc en anteriores notebooks\n",
    "\n",
    "### DataFrameWritter\n",
    "https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrameWriter\n",
    "\n",
    "Podemos guardar datos, ya sean DF, archivos, o tablas en base a diferentes formatos, para ver los argumentos del DataFrameReader en el link adjunto de la documentación de spark.\n",
    "\n",
    "Hemos utilizado el DFwriter cada vez que hemos creado un DF a través de un archivo, ya sea CSV, Parquet, Json, etc en anteriores notebooks\n",
    "\n",
    "Se pueden ver **más ejemplos de lectura en Ejs_word_cap4 y guardado de DFs en Ejs_word_cap3**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Formato Parquet\n",
    "https://sparkbyexamples.com/pyspark/pyspark-read-and-write-parquet-file/\n",
    "\n",
    "El formato columnar parquet es el formato por defecto en Spark. Es la mas soportada por diferentes plataformas y frameworks, open source y la hacen la más optimizada y eficiente, sobre todo cuando el DF contiene muchas columnas. Es recomendable guardar los DFs limpios y transformados en este formato.\n",
    "\n",
    "Parquet es guardado en un directorio que contiene la estructura de los datos, los metadatos y un numero de archivos (dependiendo del numero de particiones que tenga spark para procesar).\n",
    "\n",
    "**Es muy importante destacar que para este tipo de archivos parquet NO es necesario especificar un schema por que está incluido en los metadatos**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read mnm_df en formato parquet\n",
    "df_parquet = spark.read.format(\"parquet\").load(\"./Datasets/DFs_saved/mnm_df_parquet/*\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardar DF como parquet\n",
    "( mnm_df.write.mode('overwrite') \n",
    "            .parquet(\"./Datasets/DFs_saved/mnm_df_parquet\") )\n",
    "# *Como parquet es el formato por defecto, si no incluyes nada en .format(), este lo creará por defect como parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[Month: int, DayofMonth: int, DayOfWeek: int, FlightDate: string, Origin: string, OriginCity: string, Dest: string, DestCity: string, DepTime: int, DepDelay: double, ArrTime: int, ArrDelay: double, Cancelled: double, CancellationCode: string, Diverted: double, ActualElapsedTime: double, AirTime: double, Distance: double, CarrierDelay: double, WeatherDelay: double, NASDelay: double, SecurityDelay: double, LateAircraftDelay: double]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creamos vista temporal us_delay_flights_tbl del archivo /flightsDF_parquet\n",
    "spark.sql(\"\"\"CREATE OR REPLACE TEMPORARY VIEW us_delay_flights_tbl\n",
    "USING parquet\n",
    "OPTIONS (path \"./Datasets/DFs_saved/flightsDF_parquet\")\"\"\")\n",
    "\n",
    "# Podemos hacer sentencias sql sobre esta tabla us_delay_flights_tbl\n",
    "spark.sql(\"SELECT * FROM us_delay_flights_tbl\")#.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardar tabla us_delay_flights_tbl en formato parquet\n",
    "(df.write\n",
    ".mode(\"overwrite\")\n",
    ".saveAsTable(\"us_delay_flights_tbl\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Formatos: Json\n",
    "\n",
    "JavaScript Object Notation (JSON) tambien es un formato popular y dacil de leer y parsear comparado con XML. Consta de 2 representaciones: Single-Line Mode y Multiline Mode.\n",
    "\n",
    "En Single-Line Mode cada linea es un objeto JSON, mientras que en Multi-Line Mode el archivo entero es un objeto JSON. Hay que marcar True en option() para usar este ultimo metodo.\n",
    "\n",
    "Ejemplos para leer y guardar un archivo JSON como DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# leer archivo blogs.json\n",
    "df_json = spark.read.format(\"json\").load(\"./Datasets/blogs.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write/save df en formato json\n",
    "(df_json.write.format(\"json\")\n",
    " .mode(\"overwrite\")\n",
    " .option(\"compression\", \"snappy\")\n",
    " .save(\"./Datasets/DFs_saved/mnm_df_json\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ejemplos para leer o guardar archivos JSON como una tabla o vista SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[Campaigns: array<string>, First: string, Hits: bigint, Id: bigint, Last: string, Published: string, Url: string]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creamos vista temporal us_delay_flights_tbl del archivo blogs.json\n",
    "spark.sql(\"\"\"CREATE OR REPLACE TEMPORARY VIEW us_delay_flights_tbl\n",
    "USING json\n",
    "OPTIONS (path \"./Datasets/blogs.json\")\"\"\")\n",
    "\n",
    "# Podemos hacer sentencias sql sobre esta tabla us_delay_flights_tbl\n",
    "spark.sql(\"SELECT * FROM us_delay_flights_tbl\")#.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Formatos: CSV\n",
    "\n",
    "Los archivos de texto plano son muy comunes y popular, suelen tener la coma como separador por defecto.\n",
    "\n",
    "Ejemplos para leer y guardar un archivo CSV como DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargamos el dataframe departuredelays.csv infiriendo su schema\n",
    "df = (spark.read.format(\"csv\")\n",
    ".option(\"inferSchema\", \"true\")\n",
    ".option(\"header\", \"true\")\n",
    ".load(\"./Datasets/departuredelays.csv\"))\n",
    "\n",
    "# write/save df en formato CSV\n",
    "df.write.format(\"csv\").mode(\"overwrite\").save(\"./Datasets/df_csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ejemplos para leer o guardar archivos CSV como una tabla o vista SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creamos vista temporal us_delay_flights_tbl del archivo departuredelays.csv\n",
    "spark.sql(\"\"\"CREATE OR REPLACE TEMPORARY VIEW us_delay_flights_tbl\n",
    "USING csv\n",
    "OPTIONS (\n",
    "path \"./Datasets/departuredelays.csv*\",\n",
    "header \"true\",\n",
    "inferSchema \"true\",\n",
    "mode \"FAILFAST\" )\"\"\")\n",
    "\n",
    "# Podemos hacer sentencias sql sobre esta tabla us_delay_flights_tbl\n",
    "spark.sql(\"SELECT * FROM us_delay_flights_tbl\")#.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Formatos: Avro\n",
    "\n",
    "Avro es un formato por Apache Kafka para serializzar y deserializar. Tiene algunas ventajas como el mapeo directo a JSON, velocidad y eficiencia. Tambien es utilizado por muchos lenguajes de programacion.\n",
    "\n",
    "Ejemplos para leer y guardar un archivo avro como DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# leer archivo /flightsDF_avro\n",
    "df = spark.read.format(\"parquet\").load(\"./Datasets/DFs_saved/flightsDF_avro\")\n",
    "\n",
    "# write/save df en formato avro\n",
    "(df.write.format(\"avro\")\n",
    "   .mode(\"overwrite\")\n",
    "   .option(\"compression\", \"snappy\")\n",
    "   .save(\"./Datasets/DFs_saved/flightsDF_avro\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Formatos: ORC\n",
    "\n",
    "Es otro formato de archivo columnar optimizado adicional. \n",
    "\n",
    "Ejemplos para leer y guardar un archivo ORC como DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# leer archivo 2010-summary.orc\n",
    "file = \"./Datasets/2010-summary.orc/*\"\n",
    "df = spark.read.format(\"orc\").option(\"path\", file).load()\n",
    "\n",
    "# write/save df en formato orc\n",
    "(df.write.format(\"orc\")\n",
    ".mode(\"overwrite\")\n",
    ".option(\"compression\", \"snappy\")\n",
    ".save(\"./Datasets/DFs_saved/df_orc\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Formatos: Images\n",
    "\n",
    "Desde Spark 2.4 se introdujo como formato los archivos de imagen para dar soporte  alos entornos de machine learning y deep learning como Tensorflow o Pytorch. Es importante para estos ambitos cargar y procesar datasets de imagen en este formato\n",
    "\n",
    "Antes de cargar el archivo de tipo image hay que importar este tipo de archivos desde el paquete ml. Ejemplo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Leer archivo train_images en formato image\n",
    "from pyspark.ml import image\n",
    "\n",
    "image_dir = \"./Datasets/cctvVideosasets/learning-spark-v2/cctvVideos/train_images\"\n",
    "images_df = spark.read.format(\"image\").load(image_dir)\n",
    "images_df.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
