{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Capitulo 3: Apache Spark's Structure APIs\n",
    "\n",
    "Iremos leyendo y realizando los ejemplos del capitulo 3 del libro, complementandolo con unos actividades sobre los mismos ejemplos. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como comentamos en el anterior Script sobre los capítulos 1 y 2:\n",
    "\n",
    "##### Siempre debemos iniciar una instancia SparkSession al principio. \n",
    "\n",
    "Por lo que antes de comenzar crearemos la SparkSession correspondiente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Creamos SparkSession (IMPORTANTE cambiar el nombre si estamos en otros scripts)\n",
    "spark = SparkSession\\\n",
    "    .builder\\\n",
    "    .appName(\"LibroSpark_cap3\")\\\n",
    "    .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Schemas and Creating DataFrames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos crear los DF manualmente o Importarlos en base a diferentes formatos. Un dataframe esta organizado en columnas y filas, como en una tabla, y tiene un schema predefinido con los tipos de datos que formarán las columnas (int, double, string, etc). \n",
    "\n",
    "En cuanto al schema, a la hora de crear el DF se puede definir de dos maneras:\n",
    "- Importar/Crear el DF \"Infiriendo el schema\". De esta manera, Spark determinará, evaluando algunas fijas de nuestro DF, cuales son los tipos de datos que lo forman.\n",
    "\n",
    "- Definir el schema por nuestra cuenta, lo cual tiene algunas ventajas: Liberamos a Spark de la carga de inferlirlo, evitamos que se cree un job separado sólo para leer una porccion de los datos del archivo para acertar el schema, y podemos detectar algunos errores en la definición de los datos de manera anticipada. Podemos definirlo de dos formas:\n",
    " 1. Programáticamente\n",
    " 2. Empleando Lenduajes de Definición de Datos (DDL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Month: integer (nullable = true)\n",
      " |-- DayofMonth: integer (nullable = true)\n",
      " |-- DayOfWeek: integer (nullable = true)\n",
      " |-- FlightDate: string (nullable = true)\n",
      " |-- Origin: string (nullable = true)\n",
      " |-- OriginCity: string (nullable = true)\n",
      " |-- Dest: string (nullable = true)\n",
      " |-- DestCity: string (nullable = true)\n",
      " |-- DepTime: integer (nullable = true)\n",
      " |-- DepDelay: double (nullable = true)\n",
      " |-- ArrTime: integer (nullable = true)\n",
      " |-- ArrDelay: double (nullable = true)\n",
      " |-- Cancelled: double (nullable = true)\n",
      " |-- CancellationCode: string (nullable = true)\n",
      " |-- Diverted: double (nullable = true)\n",
      " |-- ActualElapsedTime: double (nullable = true)\n",
      " |-- AirTime: double (nullable = true)\n",
      " |-- Distance: double (nullable = true)\n",
      " |-- CarrierDelay: double (nullable = true)\n",
      " |-- WeatherDelay: double (nullable = true)\n",
      " |-- NASDelay: double (nullable = true)\n",
      " |-- SecurityDelay: double (nullable = true)\n",
      " |-- LateAircraftDelay: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Importamos archivo flights-jan-apr-2018.csv en un DF IINFIRIENDO SU SCHEMA\n",
    "flightsDF = (spark.read\n",
    "             .option(\"header\", \"true\")\n",
    "             .option(\"inferSchema\", \"true\")\n",
    "             .csv(\"./Datasets/flights-jan-apr-2018.csv\") # pon aquí la ruta en tu bucket\n",
    "            )\n",
    "# Vemos el schema inferido\n",
    "flightsDF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "\n",
    "# 1 Definimos el schema \"Programáticamente\". Utilizamos la funcion Structype y dentro StructField para cada columna\n",
    "schema_1 = StructType([StructField(\"author\", StringType(), False),\n",
    "                     StructField(\"title\", StringType(), False),\n",
    "                     StructField(\"pages\", IntegerType(), False)])\n",
    "\n",
    "# 2 Definimos el schema usando DDL\n",
    "schema_2 = \"author STRING, title STRING, pages INT\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Es importante destacar que podemos elegir cualesquiera de las dos formas de definición del schema. Incluso las 2 a la vez."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+-------+-----------------+---------+-----+--------------------+\n",
      "| Id|    First|   Last|              Url|Published| Hits|           Campaigns|\n",
      "+---+---------+-------+-----------------+---------+-----+--------------------+\n",
      "|  1|    Jules|  Damji|https://tinyurl.1| 1/4/2016| 4535| [twitter, LinkedIn]|\n",
      "|  2|   Brooke|  Wenig|https://tinyurl.2| 5/5/2018| 8908| [twitter, LinkedIn]|\n",
      "|  3|    Denny|    Lee|https://tinyurl.3| 6/7/2019| 7659|[web, twitter, FB...|\n",
      "|  4|Tathagata|    Das|https://tinyurl.4|5/12/2018|10568|       [twitter, FB]|\n",
      "|  5|    Matei|Zaharia|https://tinyurl.5|5/14/2014|40578|[web, twitter, FB...|\n",
      "+---+---------+-------+-----------------+---------+-----+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "root\n",
      " |-- Id: integer (nullable = true)\n",
      " |-- First: string (nullable = true)\n",
      " |-- Last: string (nullable = true)\n",
      " |-- Url: string (nullable = true)\n",
      " |-- Published: string (nullable = true)\n",
      " |-- Hits: integer (nullable = true)\n",
      " |-- Campaigns: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      "\n",
      "None\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "StructType(List(StructField(Id,IntegerType,true),StructField(First,StringType,true),StructField(Last,StringType,true),StructField(Url,StringType,true),StructField(Published,StringType,true),StructField(Hits,IntegerType,true),StructField(Campaigns,ArrayType(StringType,true),true)))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## EJ 3.6\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "# Definimos schema forma programatica\n",
    "schema_3_6 = StructType([\n",
    "   StructField(\"Id\", IntegerType(), False),\n",
    "   StructField(\"First\", StringType(), False),\n",
    "   StructField(\"Last\", StringType(), False),\n",
    "   StructField(\"Url\", StringType(), False),\n",
    "   StructField(\"Published\", StringType(), False),\n",
    "   StructField(\"Hits\", IntegerType(), False),\n",
    "   StructField(\"Campaigns\", ArrayType(StringType()), False)])\n",
    "\n",
    "# Definimos el schema usando DDL\n",
    "schema = \"`Id` INT, `First` STRING, `Last` STRING, `Url` STRING, `Published` STRING, `Hits` INT, `Campaigns` ARRAY<STRING>\"\n",
    "\n",
    "# Creamos los datos \n",
    "data = [[1, \"Jules\", \"Damji\", \"https://tinyurl.1\", \"1/4/2016\", 4535, [\"twitter\", \"LinkedIn\"]],\n",
    "       [2, \"Brooke\",\"Wenig\",\"https://tinyurl.2\", \"5/5/2018\", 8908, [\"twitter\", \"LinkedIn\"]],\n",
    "       [3, \"Denny\", \"Lee\", \"https://tinyurl.3\",\"6/7/2019\",7659, [\"web\", \"twitter\", \"FB\", \"LinkedIn\"]],\n",
    "       [4, \"Tathagata\", \"Das\",\"https://tinyurl.4\", \"5/12/2018\", 10568, [\"twitter\", \"FB\"]],\n",
    "       [5, \"Matei\",\"Zaharia\", \"https://tinyurl.5\", \"5/14/2014\", 40578, [\"web\", \"twitter\", \"FB\", \"LinkedIn\"]],\n",
    "       [6, \"Reynold\", \"Xin\", \"https://tinyurl.6\", \"3/2/2015\", 25568, [\"twitter\", \"LinkedIn\"]]\n",
    "      ]\n",
    "\n",
    "# Creamos el DF (eligiendo cualquiera de los dos schemas)\n",
    "blogs_df = spark.createDataFrame(data, schema)\n",
    "    \n",
    "# Inspeccionamos\n",
    "blogs_df.show(5)\n",
    "\n",
    "# Vemos el schema definido en DDL\n",
    "print(blogs_df.printSchema())\n",
    "\n",
    "# Puedes ver el schema creado en DDL de manera programatica invocando el shcema\n",
    "blogs_df.schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Columns and Expressions\n",
    "\n",
    "Las columnas en DF son conceptualmente similares a como se crean en Pandas, R o una tabla RDBMS, describen el tipo de dato de un campo. \n",
    "\n",
    "Puedes usar expresiones lógicas o matemáticas en las columnas con funciones como expr() o col(). Vemos algunos ejemplos de que podemos hacer sobre las columnas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|(Hits * 2)|\n",
      "+----------+\n",
      "|      9070|\n",
      "|     17816|\n",
      "+----------+\n",
      "only showing top 2 rows\n",
      "\n",
      "+----------+\n",
      "|(Hits * 2)|\n",
      "+----------+\n",
      "|      9070|\n",
      "|     17816|\n",
      "+----------+\n",
      "only showing top 2 rows\n",
      "\n",
      "+---+---------+-------+-----------------+---------+-----+--------------------+-----------+\n",
      "| Id|    First|   Last|              Url|Published| Hits|           Campaigns|Big Hitters|\n",
      "+---+---------+-------+-----------------+---------+-----+--------------------+-----------+\n",
      "|  1|    Jules|  Damji|https://tinyurl.1| 1/4/2016| 4535| [twitter, LinkedIn]|      false|\n",
      "|  2|   Brooke|  Wenig|https://tinyurl.2| 5/5/2018| 8908| [twitter, LinkedIn]|      false|\n",
      "|  3|    Denny|    Lee|https://tinyurl.3| 6/7/2019| 7659|[web, twitter, FB...|      false|\n",
      "|  4|Tathagata|    Das|https://tinyurl.4|5/12/2018|10568|       [twitter, FB]|       true|\n",
      "|  5|    Matei|Zaharia|https://tinyurl.5|5/14/2014|40578|[web, twitter, FB...|       true|\n",
      "|  6|  Reynold|    Xin|https://tinyurl.6| 3/2/2015|25568| [twitter, LinkedIn]|       true|\n",
      "+---+---------+-------+-----------------+---------+-----+--------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Multiplicamos columna Hits por 2 en un select con cualquiera de las dos funciones mencionadas\n",
    "blogs_df.select(expr(\"Hits * 2\")).show(2)\n",
    "blogs_df.select(col(\"Hits\") * 2).show(2)\n",
    "\n",
    "# Añadimos una nueva columna 'Big Hitters' basada en una expresión condicional (valores de Hits mayores que 10000)\n",
    "# utilizamos la funcion expr() para meter a condicion\n",
    "blogs_df.withColumn(\"Big Hitters\", (expr(\"Hits > 10000\"))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|    AuthorsId|\n",
      "+-------------+\n",
      "|  JulesDamji1|\n",
      "| BrookeWenig2|\n",
      "|    DennyLee3|\n",
      "|TathagataDas4|\n",
      "+-------------+\n",
      "only showing top 4 rows\n",
      "\n",
      "+----+\n",
      "|Hits|\n",
      "+----+\n",
      "|4535|\n",
      "|8908|\n",
      "+----+\n",
      "only showing top 2 rows\n",
      "\n",
      "+----+\n",
      "|Hits|\n",
      "+----+\n",
      "|4535|\n",
      "|8908|\n",
      "+----+\n",
      "only showing top 2 rows\n",
      "\n",
      "+----+\n",
      "|Hits|\n",
      "+----+\n",
      "|4535|\n",
      "|8908|\n",
      "+----+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Concatenar tres columnas, crear una nueva columna y mostrar la nueva columna concatenada\n",
    "blogs_df.withColumn(\"AuthorsId\", (concat(expr(\"First\"), expr(\"Last\"), expr(\"Id\")))).select(col(\"AuthorsId\")).show(4)\n",
    "\n",
    "# Estas sentencias devuelven el mismo valor, mostrando que expr es lo mismo que una llamada al método col\n",
    "blogs_df.select(expr(\"Hits\")).show(2)\n",
    "blogs_df.select(col(\"Hits\")).show(2)\n",
    "blogs_df.select(\"Hits\").show(2)\n",
    "\n",
    "# Ordenamos con Sort by la columna \"Id\" en orden descendente\n",
    " # blogs_df.sort(col(\"Id\").desc).show()\n",
    " # blogs_df.sort($\"Id\".desc).show()\n",
    "\n",
    "# Destacamos aqui que podemos referirnos a las columnas como col(\"Id\") o $\"Id\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rows (Filas)\n",
    "\n",
    "Row es un objeto generico de sparkq, conteniendo una o mas columnas en forma de colección ordenada de campos. Cada columna puede ser del mismo data type (integer, string, map, array, etc.). o no.\n",
    "\n",
    "Se puede crear una fila Row desde cero con la funcion Row() y acceder a ella usando índices (index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Reynold'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import Row\n",
    "\n",
    "#Creamos Row desde cero.\n",
    "blog_row = Row(6, \"Reynold\", \"Xin\", \"https://tinyurl.6\", 255568, \"3/2/2015\", [\"twitter\", \"LinkedIn\"])\n",
    "\n",
    "# Acceso mediante índice para elementos individuales\n",
    "blog_row[1]\n",
    "'Reynold'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tambien puedes usar los objetos Row paara crear Dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----+\n",
      "|      Authors|State|\n",
      "+-------------+-----+\n",
      "|Matei Zaharia|   CA|\n",
      "|  Reynold Xin|   CA|\n",
      "+-------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import Row\n",
    "\n",
    "# Creamos objeto row\n",
    "rows = [Row(\"Matei Zaharia\", \"CA\"), Row(\"Reynold Xin\", \"CA\")]\n",
    "\n",
    "# Creamos Df con createDataFrame\n",
    "authors_df = spark.createDataFrame(rows, [\"Authors\", \"State\"]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En la mayoría de casos, dado que los archivos serán muy frantes, será mejor práctica crear los DF definiendo su schema y cargarlos será mas rápido y eficiente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Common DataFrame Operations\n",
    "\n",
    "Vamos a usar DataFrameReader de spark para cargar un DF con el cual podemos cargar los datos en diferentes formatos JSON, CSV, Parquet, Text, Avro, ORC, etc. \n",
    "\n",
    "Para escribir un DataFrame y guardarlo en una fuente de datos en un formato particular, Spark utiliza DataFrameWriter.\n",
    "\n",
    "#### Using DataFrameReader and DataFrameWriter\n",
    "Escribir y cargar un DF con Spark es simple con Spark por que podemos cargarlo de diferentes fuentes, ya sean RDBMS, NOSQL, Streaming, etc\n",
    "\n",
    "Para comenzar cargaremos un CSV que contiene los datos de las llamadas del departamento de bomberos de San Francisco. \n",
    "como hemos comentado previamente, es más eficiente definir el schema de este archivo usando la clase DataFrameReader y sus metodos. El archivo contiene 28 columnas y mas de 4 millones de registros. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- CallNumber: integer (nullable = true)\n",
      " |-- UnitID: string (nullable = true)\n",
      " |-- IncidentNumber: integer (nullable = true)\n",
      " |-- CallType: string (nullable = true)\n",
      " |-- CallDate: string (nullable = true)\n",
      " |-- WatchDate: string (nullable = true)\n",
      " |-- CallFinalDisposition: string (nullable = true)\n",
      " |-- AvailableDtTm: string (nullable = true)\n",
      " |-- Address: string (nullable = true)\n",
      " |-- City: string (nullable = true)\n",
      " |-- Zipcode: integer (nullable = true)\n",
      " |-- Battalion: string (nullable = true)\n",
      " |-- StationArea: string (nullable = true)\n",
      " |-- Box: string (nullable = true)\n",
      " |-- OriginalPriority: string (nullable = true)\n",
      " |-- Priority: string (nullable = true)\n",
      " |-- FinalPriority: integer (nullable = true)\n",
      " |-- ALSUnit: boolean (nullable = true)\n",
      " |-- CallTypeGroup: string (nullable = true)\n",
      " |-- NumAlarms: integer (nullable = true)\n",
      " |-- UnitType: string (nullable = true)\n",
      " |-- UnitSequenceInCallDispatch: integer (nullable = true)\n",
      " |-- FirePreventionDistrict: string (nullable = true)\n",
      " |-- SupervisorDistrict: string (nullable = true)\n",
      " |-- Neighborhood: string (nullable = true)\n",
      " |-- Location: string (nullable = true)\n",
      " |-- RowID: string (nullable = true)\n",
      " |-- Delay: float (nullable = true)\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import *\n",
    "\n",
    "# Definimos el schema programaticamente\n",
    "fire_schema = StructType([StructField('CallNumber', IntegerType(), True),\n",
    "                          StructField('UnitID', StringType(), True),\n",
    "                          StructField('IncidentNumber', IntegerType(), True),\n",
    "                          StructField('CallType', StringType(), True),\n",
    "                          StructField('CallDate', StringType(), True),\n",
    "                          StructField('WatchDate', StringType(), True),\n",
    "                          StructField('CallFinalDisposition', StringType(), True),\n",
    "                          StructField('AvailableDtTm', StringType(), True),\n",
    "                          StructField('Address', StringType(), True),\n",
    "                          StructField('City', StringType(), True),\n",
    "                          StructField('Zipcode', IntegerType(), True),\n",
    "                          StructField('Battalion', StringType(), True),\n",
    "                          StructField('StationArea', StringType(), True),\n",
    "                          StructField('Box', StringType(), True),\n",
    "                          StructField('OriginalPriority', StringType(), True),\n",
    "                          StructField('Priority', StringType(), True),\n",
    "                          StructField('FinalPriority', IntegerType(), True),\n",
    "                          StructField('ALSUnit', BooleanType(), True),\n",
    "                          StructField('CallTypeGroup', StringType(), True),\n",
    "                          StructField('NumAlarms', IntegerType(), True),\n",
    "                          StructField('UnitType', StringType(), True),\n",
    "                          StructField('UnitSequenceInCallDispatch', IntegerType(), True),\n",
    "                          StructField('FirePreventionDistrict', StringType(), True),\n",
    "                          StructField('SupervisorDistrict', StringType(), True),\n",
    "                          StructField('Neighborhood', StringType(), True),\n",
    "                          StructField('Location', StringType(), True),\n",
    "                          StructField('RowID', StringType(), True),\n",
    "                          StructField('Delay', FloatType(), True)])\n",
    "\n",
    "# Leemos el CSV con DataFrameReader y su funcion spark.read.csv y indicando el schema)\n",
    "sf_fire_file = \"./Datasets/sf-fire-calls.csv\"\n",
    "# Creamos DF\n",
    "fire_df = spark.read.csv(sf_fire_file, header=True, schema=fire_schema)\n",
    "\n",
    "print(fire_df.printSchema())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como hemos contentado usamos DataframeWriter para guardarlo en diferentes formatos. \n",
    "\n",
    "Parquet es un formato que guarda el schema como parte de los metadatos de parquet, por lo que si quieres volver a leerlo en formato parquet, no será necesario indicar un schema manualmente.\n",
    "\n",
    "### Saving a DataFrame as a Parquet file or SQL table\n",
    "\n",
    "Es comun guardar el DF en formato parquet, como vista tempora, pudiendo ejecutar consultas de SQLpuro, o como tabla SQL. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- CallNumber: integer (nullable = true)\n",
      " |-- UnitID: string (nullable = true)\n",
      " |-- IncidentNumber: integer (nullable = true)\n",
      " |-- CallType: string (nullable = true)\n",
      " |-- CallDate: string (nullable = true)\n",
      " |-- WatchDate: string (nullable = true)\n",
      " |-- CallFinalDisposition: string (nullable = true)\n",
      " |-- AvailableDtTm: string (nullable = true)\n",
      " |-- Address: string (nullable = true)\n",
      " |-- City: string (nullable = true)\n",
      " |-- Zipcode: integer (nullable = true)\n",
      " |-- Battalion: string (nullable = true)\n",
      " |-- StationArea: string (nullable = true)\n",
      " |-- Box: string (nullable = true)\n",
      " |-- OriginalPriority: string (nullable = true)\n",
      " |-- Priority: string (nullable = true)\n",
      " |-- FinalPriority: integer (nullable = true)\n",
      " |-- ALSUnit: boolean (nullable = true)\n",
      " |-- CallTypeGroup: string (nullable = true)\n",
      " |-- NumAlarms: integer (nullable = true)\n",
      " |-- UnitType: string (nullable = true)\n",
      " |-- UnitSequenceInCallDispatch: integer (nullable = true)\n",
      " |-- FirePreventionDistrict: string (nullable = true)\n",
      " |-- SupervisorDistrict: string (nullable = true)\n",
      " |-- Neighborhood: string (nullable = true)\n",
      " |-- Location: string (nullable = true)\n",
      " |-- RowID: string (nullable = true)\n",
      " |-- Delay: float (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Guardar DF como parquet con modo overwrite para sobreescribir el archivo\n",
    "fire_df.write.mode('overwrite').parquet(\"./Datasets/DFs_saved/fire_df_parquet\")\n",
    "\n",
    "# Crea un archivo que termina en snappy.parquet pero está vacio y sigue dando el error\n",
    "# Snappy es el algoritmo de compresión de parquet por defecto\n",
    "\n",
    "fire_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+--------------+----------------+----------+----------+--------------------+--------------------+--------------------+----+-------+---------+-----------+----+----------------+--------+-------------+-------+-------------+---------+--------+--------------------------+----------------------+------------------+--------------------+--------------------+-------------+---------+\n",
      "|CallNumber|UnitID|IncidentNumber|        CallType|  CallDate| WatchDate|CallFinalDisposition|       AvailableDtTm|             Address|City|Zipcode|Battalion|StationArea| Box|OriginalPriority|Priority|FinalPriority|ALSUnit|CallTypeGroup|NumAlarms|UnitType|UnitSequenceInCallDispatch|FirePreventionDistrict|SupervisorDistrict|        Neighborhood|            Location|        RowID|    Delay|\n",
      "+----------+------+--------------+----------------+----------+----------+--------------------+--------------------+--------------------+----+-------+---------+-----------+----+----------------+--------+-------------+-------+-------------+---------+--------+--------------------------+----------------------+------------------+--------------------+--------------------+-------------+---------+\n",
      "|  20110016|   T13|       2003235|  Structure Fire|01/11/2002|01/10/2002|               Other|01/11/2002 01:51:...|2000 Block of CAL...|  SF|  94109|      B04|         38|3362|               3|       3|            3|  false|         null|        1|   TRUCK|                         2|                     4|                 5|     Pacific Heights|(37.7895840679362...|020110016-T13|     2.95|\n",
      "|  20110022|   M17|       2003241|Medical Incident|01/11/2002|01/10/2002|               Other|01/11/2002 03:01:...|0 Block of SILVER...|  SF|  94124|      B10|         42|6495|               3|       3|            3|   true|         null|        1|   MEDIC|                         1|                    10|                10|Bayview Hunters P...|(37.7337623673897...|020110022-M17|      4.7|\n",
      "|  20110023|   M41|       2003242|Medical Incident|01/11/2002|01/10/2002|               Other|01/11/2002 02:39:...|MARKET ST/MCALLIS...|  SF|  94102|      B03|         01|1455|               3|       3|            3|   true|         null|        1|   MEDIC|                         2|                     3|                 6|          Tenderloin|(37.7811772186856...|020110023-M41|2.4333334|\n",
      "|  20110032|   E11|       2003250|    Vehicle Fire|01/11/2002|01/10/2002|               Other|01/11/2002 04:16:...|APPLETON AV/MISSI...|  SF|  94110|      B06|         32|5626|               3|       3|            3|  false|         null|        1|  ENGINE|                         1|                     6|                 9|      Bernal Heights|(37.7388432849018...|020110032-E11|      1.5|\n",
      "|  20110043|   B04|       2003259|          Alarms|01/11/2002|01/10/2002|               Other|01/11/2002 06:01:...|1400 Block of SUT...|  SF|  94109|      B04|         03|3223|               3|       3|            3|  false|         null|        1|   CHIEF|                         2|                     4|                 2|    Western Addition|(37.7872890372638...|020110043-B04|3.4833333|\n",
      "+----------+------+--------------+----------------+----------+----------+--------------------+--------------------+--------------------+----+-------+---------+-----------+----+----------------+--------+-------------+-------+-------------+---------+--------+--------------------------+----------------------+------------------+--------------------+--------------------+-------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Creamos vista temporal a través del DF para poder usar consultas de SQLpuro\n",
    "fire_df.createOrReplaceTempView(\"fire_df_tempview\")\n",
    "\n",
    "results_fire_df = spark.sql(\"SELECT * FROM fire_df_tempview LIMIT 5\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tambien podemos guardar nuestro DF en formato parquet Particionando por una o varias columnas\n",
    " # De esta manera, se crea en el directorio la carpeta \"AlSUnit\"\n",
    "    # y subdividida en las categorias de la misma columna los datos particionados\n",
    "\n",
    "fire_df.write.partitionBy(\"ALSUnit\").mode(\"overwrite\").parquet(\"./Datasets/DFs_saved/fire_df_parquet_partition\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformations and actions\n",
    "\n",
    "Ahora que tenemos nuestro DF distribuido en memoria \"fire_df\", lo primero que haremos será examinarlo para ver cuantas filas tiene y verificar si las columnas se ven bien, si los datos están correctos, si se pueden convertir en otros tipos, si tiene valores nulos, etc. Todo esto a través de transformaciones, acciones y querys.\n",
    "\n",
    "Usamos metodos como en una query dentro select para examinar algunos aspectos especificos del dataset. A continuacion se pueden ver varios ejemplos de querys para obtener o examnidar aspectos o infformación especifica del dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+----------------------+--------------+\n",
      "|IncidentNumber|AvailableDtTm         |CallType      |\n",
      "+--------------+----------------------+--------------+\n",
      "|2003235       |01/11/2002 01:51:44 AM|Structure Fire|\n",
      "|2003250       |01/11/2002 04:16:46 AM|Vehicle Fire  |\n",
      "|2003259       |01/11/2002 06:01:58 AM|Alarms        |\n",
      "|2003279       |01/11/2002 08:03:26 AM|Structure Fire|\n",
      "|2003301       |01/11/2002 09:46:44 AM|Alarms        |\n",
      "+--------------+----------------------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Select 3 columnas cuando CallType es Medical incident con where\n",
    "\n",
    "few_fire_df = (fire_df.select(\"IncidentNumber\", \"AvailableDtTm\", \"CallType\")\n",
    "               .where(col(\"CallType\") != \"Medical Incident\"))\n",
    "\n",
    "few_fire_df.show(5, truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|DistinctCallTypes|\n",
      "+-----------------+\n",
      "|               30|\n",
      "+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import *\n",
    "# ¿Cuántos CallTypes distintos se registraron como causas de las llamadas de incendio? usando countDistinct dentro de agg\n",
    "(fire_df\n",
    ".select(\"CallType\")\n",
    ".where(col(\"CallType\").isNotNull())\n",
    ".agg(countDistinct(\"CallType\").alias(\"DistinctCallTypes\"))\n",
    ".show())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------+\n",
      "|CallType                           |\n",
      "+-----------------------------------+\n",
      "|Elevator / Escalator Rescue        |\n",
      "|Marine Fire                        |\n",
      "|Aircraft Emergency                 |\n",
      "|Confined Space / Structure Collapse|\n",
      "|Administrative                     |\n",
      "|Alarms                             |\n",
      "|Odor (Strange / Unknown)           |\n",
      "|Citizen Assist / Service Call      |\n",
      "|HazMat                             |\n",
      "|Watercraft in Distress             |\n",
      "+-----------------------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Filtramos sólo los CallTypes distintos no nulos de todas las filas\n",
    "(fire_df\n",
    ".select(\"CallType\")\n",
    ".where(col(\"CallType\").isNotNull())\n",
    ".distinct()\n",
    ".show(10, False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Renaming, adding, and dropping columns\n",
    "\n",
    "Mostramos algunos ejemplos de como renombrar (withColumnRenamed), añadir (withColumn) o eliminar (drop) columnas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+\n",
      "|ResponseDelayedinMins|\n",
      "+---------------------+\n",
      "|5.35                 |\n",
      "|6.25                 |\n",
      "|5.2                  |\n",
      "|5.6                  |\n",
      "|7.25                 |\n",
      "+---------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Creamos nuevo DF renombrando la columna \"Delay\" por \"ResponseDelayedinMins\" y \n",
    "# Hacemos un select que muestre aquellas llamadas con un delay de más de 5 minutos\n",
    "new_fire_df = fire_df.withColumnRenamed(\"Delay\", \"ResponseDelayedinMins\")\n",
    "(new_fire_df\n",
    ".select(\"ResponseDelayedinMins\")\n",
    ".where(col(\"ResponseDelayedinMins\") > 5)\n",
    ".show(5, False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------------------+-------------------+\n",
      "|IncidentDate       |OnWatchDate        |AvailableDtTS      |\n",
      "+-------------------+-------------------+-------------------+\n",
      "|2002-01-11 00:00:00|2002-01-10 00:00:00|2002-01-11 01:51:44|\n",
      "|2002-01-11 00:00:00|2002-01-10 00:00:00|2002-01-11 03:01:18|\n",
      "|2002-01-11 00:00:00|2002-01-10 00:00:00|2002-01-11 02:39:50|\n",
      "|2002-01-11 00:00:00|2002-01-10 00:00:00|2002-01-11 04:16:46|\n",
      "|2002-01-11 00:00:00|2002-01-10 00:00:00|2002-01-11 06:01:58|\n",
      "+-------------------+-------------------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Añadimos columnas \"IncidentDate\", \"OnWatchDate\", \"AvailableDtTS\" borrando sus columnas que no necesitamos despues \n",
    " # añadiendo sus datatypes, en el caso de datos timestamp añadiendo el formato de la fecha\n",
    "    \n",
    "fire_ts_df = (new_fire_df\n",
    ".withColumn(\"IncidentDate\", to_timestamp(col(\"CallDate\"), \"MM/dd/yyyy\"))\n",
    ".drop(\"CallDate\")\n",
    ".withColumn(\"OnWatchDate\", to_timestamp(col(\"WatchDate\"), \"MM/dd/yyyy\"))\n",
    ".drop(\"WatchDate\")\n",
    ".withColumn(\"AvailableDtTS\", to_timestamp(col(\"AvailableDtTm\"),\n",
    "\"MM/dd/yyyy hh:mm:ss a\"))\n",
    ".drop(\"AvailableDtTm\"))\n",
    "\n",
    "# Vemos las columnas añadidas\n",
    "(fire_ts_df\n",
    ".select(\"IncidentDate\", \"OnWatchDate\", \"AvailableDtTS\")\n",
    ".show(5, False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora que tenemos los datos tipo fecha modificados, podemos usar funciones de consulta como like month(), year(), and day() para explorar aún mñas los datos de tipo fecha. Algunos ejemplos de operacoines comunes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|year(IncidentDate)|\n",
      "+------------------+\n",
      "|              2000|\n",
      "|              2001|\n",
      "|              2002|\n",
      "|              2003|\n",
      "|              2004|\n",
      "+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Seleccinamos los años de la columna de datos IncidentDate con la guncion year, ordenandolos de manera ascendente\n",
    "(fire_ts_df\n",
    " .select(year('IncidentDate'))\n",
    " .distinct()\n",
    " .orderBy(year('IncidentDate'))\n",
    " .show(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Operaciones de agregado\n",
    "Una última operación común es agrupar datos por valores en una columna y agregar los datos de alguna manera, como simplemente contarlos o filtrarlos.\n",
    "\n",
    "Para ello, utilizamos transformaciones o acciones en DF como groupBy(), orderBy(), and count() que ofrecen la posibilidad de agregar por columna y contrar a traves de ella.\n",
    "\n",
    "Algunos ejemplos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------+------+\n",
      "|CallType                       |count |\n",
      "+-------------------------------+------+\n",
      "|Medical Incident               |113794|\n",
      "|Structure Fire                 |23319 |\n",
      "|Alarms                         |19406 |\n",
      "|Traffic Collision              |7013  |\n",
      "|Citizen Assist / Service Call  |2524  |\n",
      "|Other                          |2166  |\n",
      "|Outside Fire                   |2094  |\n",
      "|Vehicle Fire                   |854   |\n",
      "|Gas Leak (Natural and LP Gases)|764   |\n",
      "|Water Rescue                   |755   |\n",
      "+-------------------------------+------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Contamos el numero de filas no nulas de la columna CallType orenandolas por la columna count en orden ascendente\n",
    "(fire_ts_df\n",
    ".select(\"CallType\")\n",
    ".where(col(\"CallType\").isNotNull())\n",
    ".groupBy(\"CallType\")\n",
    ".count()\n",
    ".orderBy(\"count\", ascending=False)\n",
    ".show(n=10, truncate=False))\n",
    "\n",
    "# De esta consulta podemos sacar la conclusion de que tipo de llamada mas comun es por un Incidente Medico"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Other common DataFrame operations.\n",
    "\n",
    "La API DF tambien tiene algunas funciones estadisticas como min(), max(), sum() y avg(). Algunos ejemplos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------------------------+--------------------------+--------------------------+\n",
      "|sum(NumAlarms)|avg(ResponseDelayedinMins)|min(ResponseDelayedinMins)|max(ResponseDelayedinMins)|\n",
      "+--------------+--------------------------+--------------------------+--------------------------+\n",
      "|        176170|         3.892364154521585|               0.016666668|                   1844.55|\n",
      "+--------------+--------------------------+--------------------------+--------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Calculamos el total de alarmas y los valores minimos, maximos y medios de DelayResponse\n",
    "import pyspark.sql.functions as F # importamos las funciones como F\n",
    "(fire_ts_df\n",
    ".select(F.sum(\"NumAlarms\"), \n",
    "        F.avg(\"ResponseDelayedinMins\"),\n",
    "        F.min(\"ResponseDelayedinMins\"), \n",
    "        F.max(\"ResponseDelayedinMins\"))\n",
    ".show())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como puede ver, es fácil componer y encadenar consultas expresivas con la API de alto nivel y los operadores DSL y DSL. No podemos imaginar la opacidad y la ilegibilidad comparativa si hiciesemos lo mismo con los RDDs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset API\n",
    "Un Dataset es una colección fuertemente tipificada de objetos específicos del dominio que pueden ser transformados en paralelo\n",
    "utilizando operaciones funcionales o relacionales. \n",
    "\n",
    "En los lenguajes soportados por Spark, los Datasets sólo tienen sentido en Java y Scala, mientras que en Python y R sólo tienen sentido los DataFrames. Esto se debe a que Python y R no son compilados; los tipos se infieren o asignan dinámicamente durante la ejecución, no durante el tiempo de compilación.\n",
    "\n",
    "Desde Spark2.0 se unifico la API de DF y Dataset como una API estrucutrada con interfaces similares para que desarolladores no tuvieran que aprender una de las dos APIS. El Dataset API tiene una serie de ventajas como la detección de errores en tiempo de compilación entre otros que SOLO pueden ser aprovechadas por Scala, por lo que como estamos utilizando Python, seguiremos usando DF.\n",
    "\n",
    "Tenemos que tener en cuenta que en scala/java un DataFRame = Dataset<Row> (es decir un Dataset cuyo tipo de dato es generico, el Row).\n",
    "\n",
    "Recapitulando, las operaciones que podemos realizar en Datasets en Scala/Java como -filter(), map(), groupBy(), select(), take(), etc. - son similares a las de los DataFrames. En cierto modo, los Datasets son similares a los RDD en el sentido de que proporcionan una interfaz similar a sus métodos mencionados y la seguridad en tiempo de compilación, pero con una lectura mucho más fácil y una de programación orientada a objetos.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SparkSQL y su Motor Subyacente\n",
    "\n",
    "A nivel de programacion, SparkSQL permite a los desarolladores hacer querys en datos estructurados con un schema, DFs (compratibles con ANSI SQL:2003). Aparte de esto, el motor de Spark permite:\n",
    "- Unificar los componentes de Spark y permitir abstraccion en todos los leguajes que soporta sprark, simplificando el trabajo con datasets estructurados\n",
    "- Conectar con el Hive Metastore y tablas\n",
    "- Leer y escribir datos estructurados con u esquema especifico en diferentes formatos (JSON, CSV, Text, Avro, Parquet, ORC, etc.) y convertirlos en vistas o tablas temporales\n",
    "- Hacer de puente para herramientas externas via conectors de bbdd JDBC/OBDC\n",
    "- Crear querys con un plan optimicao por la JVM para su ejecución\n",
    "\n",
    "<center><img src=\"./images/SparkSQL_stack.PNG\"></center>\n",
    "\n",
    "En el core del motor de SparkSQL, se encuentran el Catalyst Optimizer y el Project Tungsten. Juntos crean las querys en DF o DF. Nos centraremos en este script en el Catalyst Optimizer para ver como optimiza las querys convirtiendolas en un plan de ejecución.\n",
    "\n",
    "##### Catalyst Optimizer\n",
    "Convierte las querys en un plan de ejecución en base a 4 fases:\n",
    "1. Analisis\n",
    "2. Optimización Lógica\n",
    "3. Plan Fisico\n",
    "4. Generacion del codigo\n",
    "\n",
    "Descacamos que independientemente del lenguaje que usemos estemos usando (SQLpuro a través de una vista temporal o DFs de Spark ya sea con R,Python,Java o Scala) spark utiliza este componente para crear de manera lógica un plan antes de ejecutar el codigo, por lo que el rendimiento es el mismo. Podemos ver un ejemplo gráfico de como el Catalyst Optimizer optimiza y transforma una query enl la siguiente iustración.\n",
    "\n",
    "<center><img src=\"./images/catalyst_optimizer.png\"></center>\n",
    "\n",
    "Vamos a ver un ejemplo utilizando una query realizada con el DF de MnM del anterior capitulo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Parsed Logical Plan ==\n",
      "'Sort ['Total DESC NULLS LAST], true\n",
      "+- Aggregate [State#208, Color#209], [State#208, Color#209, count(Count#210) AS Total#221L]\n",
      "   +- Project [State#208, Color#209, Count#210]\n",
      "      +- Relation[State#208,Color#209,Count#210] csv\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "State: string, Color: string, Total: bigint\n",
      "Sort [Total#221L DESC NULLS LAST], true\n",
      "+- Aggregate [State#208, Color#209], [State#208, Color#209, count(Count#210) AS Total#221L]\n",
      "   +- Project [State#208, Color#209, Count#210]\n",
      "      +- Relation[State#208,Color#209,Count#210] csv\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Sort [Total#221L DESC NULLS LAST], true\n",
      "+- Aggregate [State#208, Color#209], [State#208, Color#209, count(Count#210) AS Total#221L]\n",
      "   +- Relation[State#208,Color#209,Count#210] csv\n",
      "\n",
      "== Physical Plan ==\n",
      "*(3) Sort [Total#221L DESC NULLS LAST], true, 0\n",
      "+- Exchange rangepartitioning(Total#221L DESC NULLS LAST, 200), true, [id=#131]\n",
      "   +- *(2) HashAggregate(keys=[State#208, Color#209], functions=[count(Count#210)], output=[State#208, Color#209, Total#221L])\n",
      "      +- Exchange hashpartitioning(State#208, Color#209, 200), true, [id=#127]\n",
      "         +- *(1) HashAggregate(keys=[State#208, Color#209], functions=[partial_count(Count#210)], output=[State#208, Color#209, count#226L])\n",
      "            +- FileScan csv [State#208,Color#209,Count#210] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex[file:/C:/Users/franciscomanuel.medi/OneDrive - Bosonit/Escritorio/Practicas/Pra..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<State:string,Color:string,Count:int>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import *\n",
    "\n",
    "# Ejemplo pg78\n",
    "# Cargamos DataFrame de M&M\n",
    "mnm_df = spark.read.option(\"header\", \"true\") \\\n",
    "          .option(\"inferSchema\", \"true\") \\\n",
    "          .csv(\"./Datasets/mnm_dataset.csv\")\n",
    "\n",
    "# Con DFs, realizamos un select que muestre el total de colores agrupados por estado y color ordenados por Total en orden descendente\n",
    "count_mnm_df = (mnm_df\n",
    ".select(\"State\", \"Color\", \"Count\")\n",
    ".groupBy(\"State\", \"Color\")\n",
    ".agg(count(\"Count\")\n",
    ".alias(\"Total\"))\n",
    ".orderBy(\"Total\", ascending=False))\n",
    "\n",
    "# Utilizamos el metodo .explain() para ver el proceso interno que hace spark\n",
    "count_mnm_df.explain(True) #importante añadir true para hacer la info mas legible"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Fase 1: Análisis**\n",
    "\n",
    "El motor Spark SQL comienza generando un árbol de sintaxis abstracta (AST) para la consulta SQL o DataFrame. En esta fase inicial, cualquier columna o nombre de tabla se resolverá consultando un Catálogo interno, una interfaz programática de Spark SQL que contiene una lista de nombres de columnas, tipos de datos, funciones, tablas, bases de datos, etc. Una vez que se han resueltos con éxito, la consulta pasa a la siguiente fase.\n",
    "\n",
    "**Fase 2: Optimización lógica**\n",
    "\n",
    "Como muestra la ilustración anterior, esta fase comprende dos etapas internas. \n",
    " - Aplicando un enfoque de optimización **basado en reglas estándar y el optimizador de Catalyst construye primero un conjunto de planes múltiples y \n",
    " - A continuación utiliza un **optimizador basado en costes (CBO)**, asignando costes a cada plan. \n",
    "\n",
    "Estos planes se presentan como árboles de operadores pueden incluir, por ejemplo, el proceso de plegado de constantes, pushdown de predicados, poda de proyecciones, simplificación de expresiones booleanas, etc. Este plan lógico es la entrada en el plan físico.\n",
    "\n",
    "\n",
    "**Fase 3: Planificación física**\n",
    "\n",
    "En esta fase, Spark SQL genera un plan físico óptimo para el plan lógico seleccionado seleccionado, utilizando operadores físicos que coinciden con los disponibles en el motor de ejecución de Spark motor de ejecución de Spark.\n",
    "\n",
    "**Fase 4: Generación de código**\n",
    "\n",
    "La fase final de la optimización de la consulta consiste en generar un bytecode Java eficiente para ejecutar en cada máquina. Dado que Spark SQL puede operar sobre conjuntos de datos cargados en memoria, Spark puede utilizar tecnología de compilación de última generación para la generación de código para acelerar ejecución. En otras palabras, actúa como un compilador. El proyecto Tungsten, que facilita la la generación de código en toda la etapa, desempeña un papel importante.\n",
    "\n",
    "¿Qué es la generación de código en toda la fase? Es una fase de optimización de la consulta física que que reduce toda la consulta a una sola función, eliminando las llamadas a funciones virtuales y empleando los registros de la CPU para los datos intermedios. La segunda generación del motor Tungsten de segunda generación, introducido en Spark 2.0, utiliza este enfoque para generar un código RDD compacto para la ejecución final. Esta estrategia racionalizada mejora significativamente la eficiencia y el rendimiento de la CPU y el rendimiento.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
