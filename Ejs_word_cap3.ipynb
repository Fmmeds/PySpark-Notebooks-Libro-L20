{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ejercicos word Capitulo 3\n",
    "- A. Realizar todos los ejercicios propuestos de libro (HECHO En notebook Capitulo_3_ejs\n",
    "- B. Leer el CSV del ejemplo del cap2 y obtener la estructura del schema dado por defecto.\n",
    "- C. Cuando se define un schema al definir un campo por ejemplo StructField('Delay', FloatType(), True) ¿qué significa el último parámetro Boolean?\n",
    "- D. Dataset vs DataFrame (Scala). ¿En qué se diferencian a nivel de código?\n",
    "- E. Utilizando el mismo ejemplo utilizado en el capítulo para guardar en parquet y guardar los datos en los formatos: JSON, CSV (dándole otro nombre para evitar sobrescribir el fichero origen), AVRO\n",
    "\n",
    "Como siempre, iniciamos la SparkSession primero:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession \n",
    "\n",
    "# Creamos SparkSession (IMPORTANTE cambiar el nombre si estamos en otros scripts)\n",
    " # Añadimos .config con el paquete para poder usar archivos AVRO\n",
    "spark = (SparkSession\n",
    "         .builder\n",
    "         .appName(\"Ejs_word_cap3\")\n",
    "         .config('spark.jars.packages', 'org.apache.spark:spark-avro_2.12:3.0.1')  # añadimos libreria para ficheros AVRO\n",
    "         .getOrCreate())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B. Leer el CSV del ejemplo del cap2 y obtener la estructura del schema dado por defecto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+-------+\n",
      "|   State|Color|  Count|\n",
      "+--------+-----+-------+\n",
      "|20110016|  T13|2003235|\n",
      "|20110022|  M17|2003241|\n",
      "+--------+-----+-------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    " \n",
    "## Cargamos el DF del capitulo 2, el de m&m\n",
    "# Primero, definimos su schema Programaticamente\n",
    "schema_mnm_df =  StructType([StructField('State', StringType(), True),\n",
    "                             StructField('Color', StringType(), True),\n",
    "                             StructField('Count', IntegerType(), True)\n",
    "                            ])\n",
    "\n",
    "# O Definimos su schema usando DDL\n",
    "schema_mnm_df2 = \"State STRING, Color STRING, Coutn INT\"\n",
    "\n",
    "# Lo cargamos con spark.read.csv indicando primero ruta, cabecera y el schema\n",
    "mnm_df = spark.read.csv(\"./Datasets/sf-fire-calls.csv\", header=True, schema=schema_mnm_df).show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- State: string (nullable = true)\n",
      " |-- Color: string (nullable = true)\n",
      " |-- Count: integer (nullable = true)\n",
      "\n",
      "None\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "StructType(List(StructField(State,StringType,true),StructField(Color,StringType,true),StructField(Count,IntegerType,true)))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tambien podemos cargar de csv infiriendo su schema\n",
    "# Cargamos DataFrame de M&M\n",
    "mnm_df_infer = ( spark.read.format(\"csv\") \n",
    "                           .option(\"header\", \"true\")\n",
    "                           .option(\"inferSchema\", \"true\")\n",
    "                           .load(\"./Datasets/mnm_dataset.csv\") )\n",
    "\n",
    "# Vemos el schema definido en DDL\n",
    "print(mnm_df_infer.printSchema())\n",
    "\n",
    "# Puedes ver el schema creado en DDL de manera programatica invocando el shcema\n",
    "mnm_df_infer.schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C. Cuando se define un schema al definir un campo por ejemplo StructField('Delay', FloatType(), True) ¿qué significa el último parámetro Boolean?\n",
    "\n",
    "StructField('Delay', BooleanType(), True) )\n",
    "\n",
    "El data type booleano representa datos binarios (V/F, 0/1)\n",
    "\n",
    "Lista de pyspark.sql.types: \n",
    "- DataType\n",
    "- NullType\n",
    "- StringType\n",
    "- BinaryType\n",
    "- BooleanType\n",
    "- DateType\n",
    "- TimestampType\n",
    "- DecimalType\n",
    "- DoubleType\n",
    "- FloatType\n",
    "- ByteType\n",
    "- IntegerType\n",
    "- LongType\n",
    "- ShortType\n",
    "- ArrayType\n",
    "- MapType\n",
    "- StructField\n",
    "- StructType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejemplo de schema con datos boolean\n",
    "boolean_schema = StructType([ StructField('Boolean', BooleanType(), True) ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### D. Dataset vs DataFrame (Scala). ¿En qué se diferencian a nivel de código?\n",
    "Un Dataset es una colección fuertemente tipificada de objetos específicos del dominio que pueden ser transformados en paralelo\n",
    "utilizando operaciones funcionales o relacionales. \n",
    "\n",
    "Tenemos que tener en cuenta que en scala/java un DataFRame = Dataset<Row> (es decir un Dataset cuyo tipo de dato es generico, el Row).\n",
    "\n",
    "En los lenguajes soportados por Spark (R,Python,Scala y Java), los Datasets sólo tienen sentido en Java y Scala, mientras que en Python y R sólo tienen sentido los DataFrames. Esto se debe a que Python y R no son compilados; los tipos se infieren o asignan dinámicamente durante la ejecución, no durante el tiempo de compilación.\n",
    "\n",
    "Spark2.0 unifico la API de DF y Dataset como una API estrucutrada con interfaces similares para que desarolladores no tuvieran que aprender una de las dos APIS. El Dataset API tiene una serie de ventajas como la detección de errores en tiempo de compilación entre otros que SOLO pueden ser aprovechadas por Scala, por lo que como estamos utilizando Python, seguiremos usando DF.\n",
    "\n",
    "Recapitulando, las operaciones que podemos realizar en Datasets en Scala/Java como -filter(), map(), groupBy(), select(), take(), etc. - son similares a las de los DataFrames. En cierto modo, los Datasets son similares a los RDD en el sentido de que proporcionan una interfaz similar a sus métodos mencionados y la seguridad en tiempo de compilación, pero con una lectura mucho más fácil y una de programación orientada a objetos.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### E. Utilizando el mismo ejemplo utilizado en el capítulo para guardar en parquet y guardar los datos en los formatos: JSON, CSV (dándole otro nombre para evitar sobrescribir el fichero origen), AVRO\n",
    "\n",
    "Lo haremos con el DF mnm_df. \n",
    "Todos los ejemplos están en el directorio \"./Datasets/DFs_saved\".\n",
    "\n",
    "Destacamos que se crean X particiones por defecto, dividiendose el archivo en Xs archivos, podemos modificar esta forma de particionar con metodos como repartition y coalesce, por lo que reduciremos este numero de particiones a 1 para este DF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargamos DataFrame de M&M\n",
    "mnm_df = ( spark.read.format(\"csv\") \n",
    "                           .option(\"header\", \"true\")\n",
    "                           .option(\"inferSchema\", \"true\")\n",
    "                           .load(\"./Datasets/mnm_dataset.csv\").repartition(1) ) \n",
    "# añadimos repartition para que los guarde en 1 archivo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### E.1 Formato CSV \n",
    "https://sparkbyexamples.com/pyspark/pyspark-read-csv-file-into-dataframe/#write-csv-file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "# Guardar en CSV\n",
    "( mnm_df.write.mode('overwrite') \n",
    "             .options(header='True', delimiter=',') \n",
    "             .csv(\"./Datasets/DFs_saved/mnm_df_csv\") )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### E.2 Formato JSON\n",
    "https://sparkbyexamples.com/spark/spark-read-and-write-json-file/#write-json-file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardar en JSON ()\n",
    "mnm_df.write.mode('overwrite') \\\n",
    "             .json(\"./Datasets/DFs_saved/mnm_df_json\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### E.3 Formato Parquet\n",
    "https://sparkbyexamples.com/pyspark/pyspark-read-and-write-parquet-file/\n",
    "\n",
    "El formato columnar parquet es el formato por defecto en Spark. Es la mas soportada por diferentes plataformas y frameworks, open source y la hacen la más optimizada y eficiente, sobre todo cuando el DF contiene muchas columnas. Es recomendable guardar los DFs limpios y transformados en este formato.\n",
    "\n",
    "Parquet es guardado en un directorio que contiene la estructura de los datos, los metadatos y un numero de archivos (dependiendo del numero de particiones que tenga spark para procesar).\n",
    "\n",
    "**Es muy importante destacar que para este tipo de archivos parquet NO es necesario especificar un schema por que está incluido en los metadatos**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardar DF como parquet\n",
    "( mnm_df.write.mode('overwrite') \n",
    "            .parquet(\"./Datasets/DFs_saved/mnm_df_parquet\") )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### E.4 Formato AVRO\n",
    "https://sparkbyexamples.com/spark/read-write-avro-file-spark-dataframe/\n",
    "\n",
    "Destacamos que para usar archivos de formato AVRO hay que iniciar una librería externa, inciandola en la parte .config de nuestra SparkSession (mirar arrriba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.avro.functions import *\n",
    "\n",
    "# Guardar DF como AVRO\n",
    "( mnm_df.write.mode('overwrite') \n",
    "             .format(\"avro\") \n",
    "             .save(\"./Datasets/DFs_saved/mnm_df_avro\") )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### F. Investigar sobre metodos repartition y coalesce para guardar archivos en diferentes formatos\n",
    "https://sparkbyexamples.com/spark/spark-repartition-vs-coalesce/\n",
    "\n",
    "Spark divide los datos en varias particiones, ya sea a la hora de guardar los archivos en diferentes formatos (como vimos en el ejercicio anterior de guardar en CSV, que lo guarda en 10 particiones diferentes) o de leer los datos, ejecutando las operaciones en particiones de datos de forma paralela, permitiendo trabajar más rapido.\n",
    "\n",
    "A veces, será necesario aumentar o disminuir las particiones en función de la distribución de los datos, para ello usamos los metosos coalesce() y repartition().\n",
    "\n",
    "En el ejercicio anterior, Spark particionó los datos de manera predeterminada, vamos a cambiar con los metodos mencionados esto para poder obtener menos archivos a la hora de guardar/leer.\n",
    "\n",
    "Metodos: \n",
    "- getNumPartitions() muestra el numero de particiones por defecto\n",
    "- coalesce() disminuye el número de particiones de un DF o RDD de manera eficiente\n",
    "- repartition() aumenta o disminute el numero de particiones de un DF o RDD\n",
    "\n",
    "A destacar la doc de spark indica que repartition y coalesce son operaciones muy costosas, por lo que se debe intentar minimizar su uso tanto como sea posible. Vamos a guardar los archivos solo en un formato por cada metodo puesto que se crean archivos bastante pesados."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### rdd.getNumPartitions() muestra el numero de particiones por defecto\n",
    "\n",
    "Observamos que el numero de particiones por defecto que tiene el DF original con el que hicimos el ejercicio anterior es de 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# DF original\n",
    "mnm_df.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### repartition() aumenta o disminute el numero de particiones de un DF o RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usamos repartition para \n",
    "mnm_df_partition_1 = mnm_df.repartition(1)\n",
    "\n",
    "# Comprobamos que hemos configurado las particiones a 1\n",
    "mnm_df_partition_1.rdd.getNumPartitions()\n",
    "\n",
    "# Lo guardamos en formato csv y comprobamos en el explorador que se guardó en 1 archivo csv solo\n",
    "mnm_df_partition_1.write.mode('overwrite') \\\n",
    "                     .options(header='True', delimiter=',') \\\n",
    "                     .csv(\"./Datasets/DFs_saved/mnm_df_csv_partition1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####  coalesce() disminuye el número de particiones de un DF o RDD de manera eficiente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usamos repartition para \n",
    "mnm_df_coalesce_1 = mnm_df.coalesce(1)\n",
    "\n",
    "# Comprobamos que hemos configurado las particiones a 1\n",
    "mnm_df_partition_1.rdd.getNumPartitions()\n",
    "\n",
    "# Lo guardamos en formato parquet y comprobamos en el explorador que se guardó en 1 archivo parquet solo\n",
    "mnm_df_coalesce_1.write.mode('overwrite') \\\n",
    "                     .options(header='True', delimiter=',') \\\n",
    "                     .parquet(\"./Datasets/DFs_saved/mnm_df_parquet_coalesce\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Por último, en lugar de usar este tipo de métodos, se puede, al iniciar la SparkSession en modo local, añadir la siguiente linea:\n",
    "\n",
    "spark = (SparkSession\n",
    "         .builder\n",
    "         .master(\"local[1]\") # Añadiendo 1 al argumento master cada job se ejecuta en 1 particion en lugar de las 12 por defecto\n",
    "         .appName(\"Ejs_word_cap3\") \n",
    "         .config('spark.jars.packages', 'org.apache.spark:spark-avro_2.12:3.0.1') \n",
    "         .getOrCreate())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
