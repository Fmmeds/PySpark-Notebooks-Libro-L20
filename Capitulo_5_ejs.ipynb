{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Capitulo 5: Spark SQL and DataFrames: Spark SQL and DataFrames: Interacting with External Data Sources\n",
    "\n",
    "Iremos leyendo y realizando los ejemplos del capitulo 5 del libro, complementandolo con unos actividades sobre los mismos ejemplos.\n",
    "\n",
    "##### Siempre debemos iniciar una instancia SparkSession al principio. \n",
    "\n",
    "Por lo que antes de comenzar crearemos la SparkSession correspondiente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark \n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Creamos SparkSession\n",
    "spark = (SparkSession\n",
    "         .builder\n",
    "         .appName(\"LibroSpark_cap5\")\n",
    "         .master(\"local\")\n",
    "         .enableHiveSupport()\n",
    "         .getOrCreate())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En el capitulo 4 exploramos e interactuamos con las diferentes fuentes de datos y formatos que ofrece Spark, la api DF y SparkSQL. En este capitulo nos centraremos como SparkSQL permite:\n",
    "- Usar UDFs (User Define Functions) para Apache Hive y Apache Spark\n",
    "- Conectar con fuentes de datos externas JBDC y bbdd SQL como PostgreeSQL, MySQL, Tableau, MS SQL Server\n",
    "- Trabajar con tipos de datos complejos, funciones de alto nivel y operadores relacionales comunes.\n",
    "- Adicionalmente miraremos diferenicas entre lanzar querys de Spark con SparkSQL como la shell, el cliente beeline o Tableau"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User-Definded Functions\n",
    "\n",
    "Las UDFs son funciones creadas o definidas por el propio usuario, sirviendo de sistema para definir metodos SQL para operar sobre las columnas de un DF o Tabla. Se usan para definir una nueva columna basada en una función que extiende el vocabilario de SparkSQL para transformas DFo DS. Las UDFs se operan por sesión y no se guardan en el metastore.\n",
    "\n",
    "En cuanto a los Pros:\n",
    "- Se pueden utilizar por otros usuarios\n",
    "- No es necesario entender 100% su funcionamiento interno para su uso\n",
    "- Operan por cada SparkSession y no se guardan en el metastore \n",
    "\n",
    "En cuanto a los contras:\n",
    "- Su rendimiento es inferior en Python (Esto se resuelve usando Pandas/Vectorized UDFs) que en Scala o Java.\n",
    "- Spark SQL no puede optimizarlas\n",
    "- No garantiza el orden de evaluación de las subexpresiones/querys.\n",
    "\n",
    "#### Spark SQL UDFs\n",
    "Breve ejemplo de creacion de UDF de SparkSQL. Hay que tener en cuenta que las UDFs operan por sesion y no se guardan en el metastore:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.cubed(s)>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.types import *\n",
    "\n",
    "# Creamos funcion para elevar al cubo\n",
    "def cubed(s):\n",
    "    return s * s * s\n",
    "\n",
    "# Registramos nuestra UDF\n",
    "spark.udf.register(\"cubed\", cubed, LongType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+\n",
      "| id|id_cubed|\n",
      "+---+--------+\n",
      "|  1|       1|\n",
      "|  2|       8|\n",
      "|  3|      27|\n",
      "|  4|      64|\n",
      "|  5|     125|\n",
      "|  6|     216|\n",
      "|  7|     343|\n",
      "|  8|     512|\n",
      "+---+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Generate temporary view de ejemplo\n",
    "spark.range(1, 9).createOrReplaceTempView(\"udf_test\")\n",
    "\n",
    "# Ahora podemos usar nuestra UDF cubed() con SparkSQL \n",
    "spark.sql(\"SELECT id, cubed(id) AS id_cubed FROM udf_test \").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Speeding up and distributing PySpark UDFs with Pandas UDFs\n",
    "\n",
    "Una de los problemas que prevalecian con el uso de UDFs en Pyspark era que su rendimiento es mas lento que en Scala. Esto se debe a que PySpark UDFs tienen que mover los datos entre la JVM y Python, lo que es caro en terminos de memoria.\n",
    "\n",
    "Para resolver esto se utilizan las **Pandas UDFs o Vectorized UDFs**. Desde Spark 3.0 y Python 3.6 Pandas UDFs fue divido en 2 categorías:\n",
    "- Pandas UDFs. \n",
    "- Pandas Function APIs. Permiten udar una funcion local de Python a un DF de Pyspark donde la entrada y la salida son instancia de pandas. En Spark 3.0 soportan las funciones grouped map, map, cogrouped map.\n",
    "\n",
    "A diferencia de una función local, el uso de una UDF vectorizada **dará lugar a la ejecución de trabajos de Spark**; la función local anterior es una función de Pandas ejecutada sólo en el controlador de Spark.\n",
    "\n",
    "Ejemplo de Pandas UDFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importamos varias funciones pyspark SQL incluida pandas_udf\n",
    "import pandas as pd\n",
    "from pyspark.sql.functions import col, pandas_udf\n",
    "from pyspark.sql.types import LongType\n",
    "\n",
    "# Creamos funcion cubed para elevar numeros al cubo\n",
    "def cubed(a: pd.Series) -> pd.Series:\n",
    "    return a * a * a\n",
    "\n",
    "# Creamos la UDF pandas/vectorizada para nuestra funcion cubed creada\n",
    "# cubed_udf = pandas_udf(cubed, returnType=LongType())\n",
    "\n",
    "# ImportError: PyArrow >= 0.15.1 must be installed; however, it was not found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0     1\n",
      "1     8\n",
      "2    27\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Creamos DF Pandas Series\n",
    "x = pd.Series([1, 2, 3])\n",
    "\n",
    "# Usamos la funcion cubed\n",
    "print(cubed(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creamos Spark DF\n",
    "df = spark.range(1, 4)\n",
    "\n",
    "# Ejecutamos UDF pandas/vectorizada \n",
    "# df.select(\"id\", cubed_udf(col(\"id\"))).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Querying with the Spark SQL Shell, Beeline, and Tableau\n",
    "\n",
    "Hay varias formas de lanzar Querys en Spark como la Spark SQL Shell, el cliente Beeline, aplicaciones de reporting como Tableau o PowerBI, Jupyer notebook como es este caso,etc.\n",
    "\n",
    "En este caso veremos un ejemplo en \"Ejs_word_cap5\" que muestra el uso de conectores JBDC para usar una RDBMS con MySQL\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Higher-Order Functions\n",
    "\n",
    "Este tipo de funciones se utilizan para los tipos de datos complejos (que son abstracciones de tipos de datos ismples), tenemos la gentaja de que nos permita pensar en un porblema de manera tabular aun usando tipos de datos complejos.\n",
    "\n",
    "Aparte de las funciones mencionadas anteriormente, existen funciones de alto nivel que toman funciones anonimas formadas por expresiones lambda como argumentos. Como por ejemplo, en SQL:\n",
    "\n",
    "--SQL\n",
    "transform(values, value -> lambda expression)\n",
    "\n",
    "La función tranform() coje un array (values) y una función anónima (una expresión lambda) como entrada. La función crea de forma transparente un nuevo array aplicando la función anónima a cada elemento, y luego asignando el resultado al array de salida, este proceso es similar al enfoque UDF, pero más eficiente.\n",
    "\n",
    "Probaremos algunas hihg order functions, en primer lugar creamos nuestro DF de ejepmlo t_c y una vista temporal tC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|             celsius|\n",
      "+--------------------+\n",
      "|[35, 36, 32, 30, ...|\n",
      "|[31, 32, 34, 55, 56]|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import *\n",
    "\n",
    "# Creamos el schema primero (una unica columna celsius de tipo array int)\n",
    "schema = StructType([StructField(\"celsius\", ArrayType(IntegerType()))])\n",
    "# Creamos lista de datos (dos arrays de numeros enteros)\n",
    "t_list = [[35, 36, 32, 30, 40, 42, 38]], [[31, 32, 34, 55, 56]]\n",
    "# Creamos DF\n",
    "t_c = spark.createDataFrame(t_list, schema)\n",
    "# Creamos vista temporal a través del DF\n",
    "t_c.createOrReplaceTempView(\"tC\")\n",
    "\n",
    "# lo mostramos para verificar que está correcto\n",
    "t_c.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *Función transform()*\n",
    "Esta funcion produce un array aplicando una función a cada elemento de la matriz/array (similar a la función map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|             celsius|          fahrenheit|\n",
      "+--------------------+--------------------+\n",
      "|[35, 36, 32, 30, ...|[95, 96, 89, 86, ...|\n",
      "|[31, 32, 34, 55, 56]|[87, 89, 93, 131,...|\n",
      "+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Transformamos nuestra columna de temperatura celsius a farenheit con nuestra vista temporal tC para hacerlo en SQLpuro\n",
    "# Creamos nueva columna farenheit utilizando la función transform\n",
    "# Para ello usamos la funcion transform aplicando la formula de pasar de grados celsius a farenheit\n",
    "\n",
    "spark.sql(\"\"\" SELECT celsius, transform(celsius, t -> ((t * 9) div 5) + 32) as fahrenheit\n",
    "              FROM tC \"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *Función filter()*\n",
    "Esta funcion produce una matriz que contiene sólo en los elementos de la matriz de entrada para los que se cumole una condicion booleana que es verdadera."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------+\n",
      "|             celsius|    high|\n",
      "+--------------------+--------+\n",
      "|[35, 36, 32, 30, ...|[40, 42]|\n",
      "|[31, 32, 34, 55, 56]|[55, 56]|\n",
      "+--------------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Utilizamos filter aplicando un filtro a t que coge solo la temperaturas mayores que 38\n",
    "spark.sql(\"\"\" SELECT celsius, filter(celsius, t -> t > 38) as high\n",
    "              FROM tC \"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *Función exists()*\n",
    "Esta funcion devuelve true si la función booleana es válida para cualquier elemento de la matriz de entrada:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------+\n",
      "|             celsius|threshold|\n",
      "+--------------------+---------+\n",
      "|[35, 36, 32, 30, ...|     true|\n",
      "|[31, 32, 34, 55, 56]|    false|\n",
      "+--------------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Utilizamos exists para ver si hay tempraturas mayores de 30 en cada array/matriz en una nueva columna thresold\n",
    "spark.sql(\"\"\" SELECT celsius, exists(celsius, t -> t = 38) as threshold\n",
    "              FROM tC \"\"\").show()\n",
    "\n",
    "# como podemos observar, para el primer array si existen t>38 mientras que para el segundo array no"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *Función reduce()*\n",
    "Esta funcion reduce los elementos del array a un unico valor fusionando los elementos y aplicando una función a los mismos\n",
    "(fusionando los elementos en un buffer B usando la función<B, T, B> y aplicando una función unción<B, R> en el buffer final)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "Undefined function: 'reduce'. This function is neither a registered temporary function nor a permanent function registered in the database 'default'.; line 1 pos 17",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-29-0d2193498763>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Utilizamos reduce para aplicar la media de todos los valores del array a la vez que los pasamos a farenheit con una nueva columna avgFarenheit\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m spark.sql(\"\"\" SELECT celsius, reduce(celsius, \n\u001b[0m\u001b[0;32m      3\u001b[0m                      0, (t, acc) -> t + acc,acc -> (acc div size(celsius) * 9 div 5) + 32) as avgFahrenheit\n\u001b[0;32m      4\u001b[0m                      FROM tC \"\"\").show()\n",
      "\u001b[1;32mc:\\users\\franciscomanuel.medi\\anaconda3\\lib\\site-packages\\pyspark\\sql\\session.py\u001b[0m in \u001b[0;36msql\u001b[1;34m(self, sqlQuery)\u001b[0m\n\u001b[0;32m    647\u001b[0m         \u001b[1;33m[\u001b[0m\u001b[0mRow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34mu'row1'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34mu'row2'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34mu'row3'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    648\u001b[0m         \"\"\"\n\u001b[1;32m--> 649\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jsparkSession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msql\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msqlQuery\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_wrapped\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    650\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    651\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0msince\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2.0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\franciscomanuel.medi\\anaconda3\\lib\\site-packages\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1302\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1304\u001b[1;33m         return_value = get_return_value(\n\u001b[0m\u001b[0;32m   1305\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0;32m   1306\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\franciscomanuel.medi\\anaconda3\\lib\\site-packages\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    132\u001b[0m                 \u001b[1;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    133\u001b[0m                 \u001b[1;31m# JVM exception message.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 134\u001b[1;33m                 \u001b[0mraise_from\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconverted\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    135\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    136\u001b[0m                 \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\franciscomanuel.medi\\anaconda3\\lib\\site-packages\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mraise_from\u001b[1;34m(e)\u001b[0m\n",
      "\u001b[1;31mAnalysisException\u001b[0m: Undefined function: 'reduce'. This function is neither a registered temporary function nor a permanent function registered in the database 'default'.; line 1 pos 17"
     ]
    }
   ],
   "source": [
    "# Utilizamos reduce para aplicar la media de todos los valores del array a la vez que los pasamos a farenheit con una nueva columna avgFarenheit\n",
    "spark.sql(\"\"\" SELECT celsius, reduce(celsius, \n",
    "                     0, (t, acc) -> t + acc,acc -> (acc div size(celsius) * 9 div 5) + 32) as avgFahrenheit\n",
    "                     FROM tC \"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Common DataFrames and Spark SQL Operations\n",
    "\n",
    "Parte del poder de SparkSQL viene de el amplio rango de operacione con DF que soporta, podemos engobar estas operaciones o funciones más comunes en:\n",
    "- Funciones de agregado\n",
    "- Funciones de colección\n",
    "- Funciones para datos de tipo fecha\n",
    "- Funcioines matemáticas\n",
    "- Funciones varias (miscelanea)\n",
    "- Funciones no agregadas\n",
    "- Funciones de agregado\n",
    "- Funciones para datos de tipo string\n",
    "- Funciones UDF\n",
    "- Funciones de ventana\n",
    "\n",
    "Se puede consultar la lista completa en la documentación de SparkSQL\n",
    "https://spark.apache.org/docs/latest/api/sql/index.html\n",
    "\n",
    "En esta sección nos en estos tipos de funciones y operaciones relacionales:\n",
    "- Uniones y Joins\n",
    "- Ventana\n",
    "- Modificaciones\n",
    "\n",
    "Para llevar acabo estas operaciones y ejemplos, prepararemos un DF en el siguiente codigo de ejemplo:\n",
    "\n",
    "1. Importe dos archivos y cree dos DataFrames, uno para la **información del aeropuerto (airportsna)** y otro para los **retrasos de los vuelos de EEUU (departureDelays)**.\n",
    "\n",
    "2. Utilizando expr(), **convierta las columnas de retrasos y distancias de STRING a INT.**\n",
    "\n",
    "3. Cree una **tabla más pequeña, foo**, en la que podamos centrarnos para nuestros ejemplos de demostración; sólo contiene **información sobre tres vuelos con origen en Seattle (SEA) y destino en San Francisco (SFO) para un pequeño intervalo de tiempo.**\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Importe dos archivos y cree dos DataFrames, uno para la información del aeropuerto (airportsna) y otro para los retrasos de los vuelos de EEUU (departureDelays)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+-------+----+\n",
      "|      City|State|Country|IATA|\n",
      "+----------+-----+-------+----+\n",
      "|Abbotsford|   BC| Canada| YXX|\n",
      "|  Aberdeen|   SD|    USA| ABR|\n",
      "|   Abilene|   TX|    USA| ABI|\n",
      "|     Akron|   OH|    USA| CAK|\n",
      "|   Alamosa|   CO|    USA| ALS|\n",
      "+----------+-----+-------+----+\n",
      "only showing top 5 rows\n",
      "\n",
      "+--------+-----+--------+------+-----------+\n",
      "|    date|delay|distance|origin|destination|\n",
      "+--------+-----+--------+------+-----------+\n",
      "|01011245|    6|     602|   ABE|        ATL|\n",
      "|01020600|   -8|     369|   ABE|        DTW|\n",
      "|01021245|   -2|     602|   ABE|        ATL|\n",
      "|01020605|   -4|     602|   ABE|        ATL|\n",
      "|01031245|   -4|     602|   ABE|        ATL|\n",
      "+--------+-----+--------+------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import expr\n",
    "    \n",
    "# Marcamos las rutas de los 2 datasets (departuredelays.csv y airport-codes-na.txt)\n",
    "tripdelaysFilePath = \"./Datasets/departuredelays.csv\"\n",
    "airportsnaFilePath = \"./Datasets/airport-codes-na.txt\"\n",
    "\n",
    "# Creamos airportsna a partir de airport-codes-na y creamos una vista temporal\n",
    "airportsna = (spark.read.format(\"csv\")\n",
    "                   .options(header=\"true\", inferSchema = \"true\", sep=\"\\t\")\n",
    "                   .load(airportsnaFilePath))\n",
    "\n",
    "airportsna.createOrReplaceTempView(\"airports_na\")\n",
    "\n",
    "# Creamos departureDelays a partir de departuredelays.csv y creamos una vista temporal\n",
    "departureDelays = (spark.read.format(\"csv\")\n",
    "                         .options(header=\"true\")\n",
    "                         .load(tripdelaysFilePath))\n",
    "\n",
    "# verificamos que todo se ha creado bien\n",
    "airportsna.show(5)\n",
    "departureDelays.show(5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Utilizando expr(), convierta las columnas de retrasos y distancias de STRING a INT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modificamos las columnas delay y distance transformadolas a datatype int en el DF departureDelays\n",
    " # para ello utilizamos expr\n",
    "departureDelays = (departureDelays.withColumn(\"delay\", expr(\"CAST(delay as INT) as delay\"))\n",
    "                                  .withColumn(\"distance\", expr(\"CAST(distance as INT) as distance\")))\n",
    "\n",
    "# Creamos su correspondiente vista temporal\n",
    "departureDelays.createOrReplaceTempView(\"departureDelays\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Cree una tabla más pequeña, foo, en la que podamos centrarnos para nuestros ejemplos de demostración; sólo contiene información sobre tres vuelos con origen en Seattle (SEA) y destino en San Francisco (SFO) para un pequeño intervalo de tiempo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create tabla temporal foo\n",
    "# Esta subtabla coniene la información de los vuelos con origen en seattle y san francisco para un pequeño intervalo de tiempo\n",
    "foo = (departureDelays.filter(expr(\"\"\"origin == 'SEA' and destination == 'SFO' and\n",
    "                                      date like '01010%' and delay > 0\"\"\")))\n",
    "\n",
    "foo.createOrReplaceTempView(\"foo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El DF departureDelays contiene más de 1.3M de vuelos mientras que el DF foo contiene solo 3 vuelos de SEA y SFO para un rango específico de tiempo. Podemos ver todo esto en el siguiente ejemplo en el que verificamos que todo ha sido creado correctamente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----+-------+----+\n",
      "|       City|State|Country|IATA|\n",
      "+-----------+-----+-------+----+\n",
      "| Abbotsford|   BC| Canada| YXX|\n",
      "|   Aberdeen|   SD|    USA| ABR|\n",
      "|    Abilene|   TX|    USA| ABI|\n",
      "|      Akron|   OH|    USA| CAK|\n",
      "|    Alamosa|   CO|    USA| ALS|\n",
      "|     Albany|   GA|    USA| ABY|\n",
      "|     Albany|   NY|    USA| ALB|\n",
      "|Albuquerque|   NM|    USA| ABQ|\n",
      "| Alexandria|   LA|    USA| AEX|\n",
      "|  Allentown|   PA|    USA| ABE|\n",
      "+-----------+-----+-------+----+\n",
      "\n",
      "+--------+-----+--------+------+-----------+\n",
      "|    date|delay|distance|origin|destination|\n",
      "+--------+-----+--------+------+-----------+\n",
      "|01011245|    6|     602|   ABE|        ATL|\n",
      "|01020600|   -8|     369|   ABE|        DTW|\n",
      "|01021245|   -2|     602|   ABE|        ATL|\n",
      "|01020605|   -4|     602|   ABE|        ATL|\n",
      "|01031245|   -4|     602|   ABE|        ATL|\n",
      "|01030605|    0|     602|   ABE|        ATL|\n",
      "|01041243|   10|     602|   ABE|        ATL|\n",
      "|01040605|   28|     602|   ABE|        ATL|\n",
      "|01051245|   88|     602|   ABE|        ATL|\n",
      "|01050605|    9|     602|   ABE|        ATL|\n",
      "+--------+-----+--------+------+-----------+\n",
      "\n",
      "+--------+-----+--------+------+-----------+\n",
      "|    date|delay|distance|origin|destination|\n",
      "+--------+-----+--------+------+-----------+\n",
      "|01010710|   31|     590|   SEA|        SFO|\n",
      "|01010955|  104|     590|   SEA|        SFO|\n",
      "|01010730|    5|     590|   SEA|        SFO|\n",
      "+--------+-----+--------+------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Utilizamos spark.sql para hacer un select rapido en sqlpuro para ver que el contenido está bien\n",
    "spark.sql(\"SELECT * FROM airports_na LIMIT 10\").show()\n",
    "spark.sql(\"SELECT * FROM departureDelays LIMIT 10\").show()\n",
    "spark.sql(\"SELECT * FROM foo\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez tenemos todos nuestros datos limpios y preparados, realizamos unos ejemplos de operaciones/funciones relacionales:\n",
    "- union y joins\n",
    "- ventana\n",
    "- modificaciones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Union\n",
    "Esta operación une dos DF diretentes con un el mismo schema. Para ello utilizamos el metodo union()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+--------+------+-----------+\n",
      "|    date|delay|distance|origin|destination|\n",
      "+--------+-----+--------+------+-----------+\n",
      "|01010710|   31|     590|   SEA|        SFO|\n",
      "|01010955|  104|     590|   SEA|        SFO|\n",
      "|01010730|    5|     590|   SEA|        SFO|\n",
      "|01010710|   31|     590|   SEA|        SFO|\n",
      "|01010955|  104|     590|   SEA|        SFO|\n",
      "|01010730|    5|     590|   SEA|        SFO|\n",
      "+--------+-----+--------+------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Unimos los DF departureDelays y foo \n",
    "bar = departureDelays.union(foo)\n",
    "\n",
    "# Creamos una tabla temporal bar\n",
    "bar.createOrReplaceTempView(\"bar\")\n",
    "\n",
    "# Mostramos la union filtrando or SEA and SFO en un rango de tiempo específico\n",
    "bar.filter(expr(\"\"\"origin == 'SEA' AND destination == 'SFO' AND date LIKE '01010%' AND delay > 0\"\"\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Joins\n",
    "Esta operación join une dos DF diretentes por COLUMNAS con un el mismo schema. Para ello utiliza el metodo join(). Por defecto, SparkSQL utiliza el join de tipo \"inner join\", aunque puedes cambiar a otros tipos de joins como:\n",
    "- being inner \n",
    "- cross\n",
    "- outer \n",
    "- full \n",
    "- full_outer \n",
    "- left \n",
    "- left_outer \n",
    "- right \n",
    "- right_outer \n",
    "- left_semi \n",
    "- oleft_anti\n",
    "\n",
    "En el siguiente ejemplo realizamos un iner join uniendo los DF airportsna y foo, en lenguaje DF y SQLpuro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+--------+-----+--------+-----------+\n",
      "|   City|State|    date|delay|distance|destination|\n",
      "+-------+-----+--------+-----+--------+-----------+\n",
      "|Seattle|   WA|01010710|   31|     590|        SFO|\n",
      "|Seattle|   WA|01010955|  104|     590|        SFO|\n",
      "|Seattle|   WA|01010730|    5|     590|        SFO|\n",
      "+-------+-----+--------+-----+--------+-----------+\n",
      "\n",
      "+-------+-----+--------+-----+--------+-----------+\n",
      "|   City|State|    date|delay|distance|destination|\n",
      "+-------+-----+--------+-----+--------+-----------+\n",
      "|Seattle|   WA|01010710|   31|     590|        SFO|\n",
      "|Seattle|   WA|01010955|  104|     590|        SFO|\n",
      "|Seattle|   WA|01010730|    5|     590|        SFO|\n",
      "+-------+-----+--------+-----+--------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Join en lenguaje DF por las columnas IATA y origin\n",
    "( foo.join(airportsna, airportsna.IATA == foo.origin)\n",
    "    .select(\"City\", \"State\", \"date\", \"delay\", \"distance\", \"destination\").show() )\n",
    "    \n",
    "# Join en lenguaje SQLpuro con spark.sql por las columnas IATA y origin\n",
    "spark.sql(\"\"\" SELECT a.City, a.State, f.date, f.delay, f.distance, f.destination\n",
    "              FROM foo f\n",
    "              JOIN airports_na a\n",
    "              ON a.IATA = f.origin \"\"\").show()\n",
    "\n",
    "# La query consiste en mostrar las columnas fecha, el retraso, la distancia \n",
    "# y el destino del DataFrame de foo unida a la información de la ciudad y el estado de el DataFrame de los aeropuertos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Windowing\n",
    "\n",
    "Una función de ventana utiliza los valores de las filas de una ventana (conjunto de filas de entrada) para devolver un conjunto de valores en otra fila. Gracias a ellas es poosible operar en un grupo de filas mientras se devuelve un unico valor para cada fila de entrada. \n",
    "\n",
    "En el siguiente ejemlo mostramos como utilizar la funcion de ventana dense_rank(). \n",
    "\n",
    "En primer lugar creamos una nueva tabla temp departureDelaysWindow creando la columna TotalDelays (calculada son sum(Delay)) experimentados por los vuelos de origen SEA, SFO y JFK que van a un conjuto de destinos (WHERE) que se indica en la siguiente query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creamos tabla departureDelaysWindow como CTA haciendo un seelect de la tabla departureDelaysWindow\n",
    "# creando la columna TotalDelays \n",
    "spark.sql ( \"\"\"DROP TABLE IF EXISTS departureDelaysWindow\"\"\" )\n",
    "\n",
    "spark.sql ( \"\"\"CREATE TABLE departureDelaysWindow AS\n",
    "            SELECT origin, destination, SUM(delay) AS TotalDelays\n",
    "            FROM departureDelays\n",
    "            WHERE origin IN ('SEA', 'SFO', 'JFK') AND destination IN ('SEA', 'SFO', 'JFK', 'DEN', 'ORD', 'LAX', 'ATL')\n",
    "            GROUP BY origin, destination\"\"\" )\n",
    "\n",
    "spark.sql ( \"\"\"SELECT * FROM departureDelaysWindow\"\"\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora queremos buscar los 3 destinos que tienen más retrasos, se puede lograr haciendo 3 consultas diferentes y luego uniendo los resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "spark.sql (  \"\"\"SELECT origin, destination, SUM(TotalDelays) AS TotalDelays\n",
    "             FROM departureDelaysWindow\n",
    "             WHERE origin = '[ORIGIN]'\n",
    "             GROUP BY origin, destination\n",
    "             ORDER BY SUM(TotalDelays) DESC\n",
    "             LIMIT 3\"\"\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O puedes hacerlo utilizando la función dense_rank para desarrollar este calculo. \n",
    "Así, con la fucnión dense_rank podemos ver un ranking de los 3 vuelos con más retraso para cada orgien.\n",
    "\n",
    "Es importante mencionar que cada función de ventana necesita caber en un solo executor y estará compuesta por una sola partición durante la ejecución. Por lo que, tenemos que asegurarnos de que sus querys no sean ilimitadas, es decir, limitamos cada tamaño de cada ventana."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "## utilizamos funcion dense_rank\n",
    "spark.sql(\"\"\" SELECT origin, destination, TotalDelays, rank\n",
    "              FROM ( \n",
    "                SELECT origin, \n",
    "                       destination, \n",
    "                       TotalDelays, \n",
    "                       dense_rank() \n",
    "                       OVER (PARTITION BY origin ORDER BY TotalDelays DESC) as rank\n",
    "                     FROM departureDelaysWindow) t\n",
    "              WHERE rank <= 3\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Modifications\n",
    "\n",
    "Otra operación comun es desarrollar modificaciones en el DF. Como el DF es inmutable, realizas modificaciones en nuevo DF o sobreescribiendo el mismo. \n",
    "\n",
    "Utilizaremos el DF foo para estos ejemplos y llevaremos a cabo\n",
    "- Añadir columnas\n",
    "- Borrar columnas\n",
    "- Renombrar columnas\n",
    "- Pivotar\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+--------+------+-----------+\n",
      "|    date|delay|distance|origin|destination|\n",
      "+--------+-----+--------+------+-----------+\n",
      "|01010710|   31|     590|   SEA|        SFO|\n",
      "|01010955|  104|     590|   SEA|        SFO|\n",
      "|01010730|    5|     590|   SEA|        SFO|\n",
      "+--------+-----+--------+------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "foo.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Añadir columnas\n",
    "\n",
    "Para añadir columnas utilizamos el metodo withColumn(). En el ejemplo crearemos el DF foo2 en el que añadimos la columna status definida con una sentencia case utilizando la función expr()\n",
    "\n",
    "El DF foo2 contiene el contenido de foo y la columna adicional status. \n",
    "Esta columna utiliza la función expre para poner la sentencia case de que cuando la columna delay sea inferior a 10 poner el status \"On-time\", en caso contrario poner \"Delayed\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+--------+------+-----------+-------+\n",
      "|    date|delay|distance|origin|destination| status|\n",
      "+--------+-----+--------+------+-----------+-------+\n",
      "|01010710|   31|     590|   SEA|        SFO|Delayed|\n",
      "|01010955|  104|     590|   SEA|        SFO|Delayed|\n",
      "|01010730|    5|     590|   SEA|        SFO|On-time|\n",
      "+--------+-----+--------+------+-----------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import *\n",
    "foo2 = ( foo.withColumn( \"status\",\n",
    "                        expr(\"CASE WHEN delay <= 10 THEN 'On-time' ELSE 'Delayed' END\") \n",
    "                       ) ).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Borrar columnas\n",
    "\n",
    "Para borrar columnas utlizamos el metodo drop(). En el ejemplo elminamos la columa delay ya que tenemos la columna status del ejemplo anterior que nos da la información de si un vuelo se retrasó o no de manera mas legible.\n",
    "\n",
    "Creamos un nuevo DF foo3 con todos estos cambios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'drop'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-63-afa7dd2d2d59>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mfoo3\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfoo2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"delay\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'drop'"
     ]
    }
   ],
   "source": [
    "foo3 = foo2.drop(\"delay\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Renombrar columnas\n",
    "\n",
    "Puedes renombrar columnas con el metodo rename() seguido del metodo withColumn().\n",
    "\n",
    "Creamos un nuevo DF foo4 con estos cambios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'withColumnRenamed'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-61-19ba4fd24e89>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mfoo4\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfoo2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwithColumnRenamed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"status\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"flight_status\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mfoo4\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'withColumnRenamed'"
     ]
    }
   ],
   "source": [
    "foo4 = foo2.withColumnRenamed(\"status\", \"flight_status\")\n",
    "foo4.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Pivotar\n",
    "\n",
    "Pivotar significa cambiar las columnas por las filas. Veamoslo en el siguiente ejemplo:\n",
    "\n",
    "Pivotar permite colocar nombres en la columna del mes (en lugar de 1 y 2 puede mostrar enero y febrero, respectivamente), así como realizar cálculos agregados (en este caso promedio y máximo) sobre los retrasos por destino y mes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----+-----+\n",
      "|destination|month|delay|\n",
      "+-----------+-----+-----+\n",
      "|        ORD|    1|   92|\n",
      "|        JFK|    1|   -7|\n",
      "|        DFW|    1|   -5|\n",
      "|        MIA|    1|   -3|\n",
      "|        DFW|    1|   -3|\n",
      "|        DFW|    1|    1|\n",
      "|        ORD|    1|  -10|\n",
      "|        DFW|    1|   -6|\n",
      "|        DFW|    1|   -2|\n",
      "|        ORD|    1|   -3|\n",
      "|        ORD|    1|    0|\n",
      "|        DFW|    1|   23|\n",
      "|        DFW|    1|   36|\n",
      "|        ORD|    1|  298|\n",
      "|        JFK|    1|    4|\n",
      "|        DFW|    1|    0|\n",
      "|        MIA|    1|    2|\n",
      "|        DFW|    1|    0|\n",
      "|        DFW|    1|    0|\n",
      "|        ORD|    1|   83|\n",
      "+-----------+-----+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# vemos en este selct que los meses se muestran en numeros (importante pasar la columna a srting)\n",
    "spark.sql ( \"\"\"SELECT destination, CAST(SUBSTRING(date, 0, 2) AS int) AS month, delay\n",
    "            FROM departureDelays\n",
    "            WHERE origin = 'SEA'\"\"\" ).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------------+------------+------------+------------+\n",
      "|destination|JAN_AvgDelay|JAN_MaxDelay|FEB_AvgDelay|FEB_MaxDelay|\n",
      "+-----------+------------+------------+------------+------------+\n",
      "|        ABQ|       19.86|         316|       11.42|          69|\n",
      "|        ANC|        4.44|         149|        7.90|         141|\n",
      "|        ATL|       11.98|         397|        7.73|         145|\n",
      "|        AUS|        3.48|          50|       -0.21|          18|\n",
      "|        BOS|        7.84|         110|       14.58|         152|\n",
      "|        BUR|       -2.03|          56|       -1.89|          78|\n",
      "|        CLE|       16.00|          27|        null|        null|\n",
      "|        CLT|        2.53|          41|       12.96|         228|\n",
      "|        COS|        5.32|          82|       12.18|         203|\n",
      "|        CVG|       -0.50|           4|        null|        null|\n",
      "|        DCA|       -1.15|          50|        0.07|          34|\n",
      "|        DEN|       13.13|         425|       12.95|         625|\n",
      "|        DFW|        7.95|         247|       12.57|         356|\n",
      "|        DTW|        9.18|         107|        3.47|          77|\n",
      "|        EWR|        9.63|         236|        5.20|         212|\n",
      "|        FAI|        1.84|         160|        4.21|          60|\n",
      "|        FAT|        1.36|         119|        5.22|         232|\n",
      "|        FLL|        2.94|          54|        3.50|          40|\n",
      "|        GEG|        2.28|          63|        2.87|          60|\n",
      "|        HDN|       -0.44|          27|       -6.50|           0|\n",
      "+-----------+------------+------------+------------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql (\"\"\" SELECT * FROM (\n",
    "               SELECT destination, CAST(SUBSTRING(date, 0, 2) AS int) AS month, delay\n",
    "                 FROM departureDelays WHERE origin = 'SEA'\n",
    "           )\n",
    "               PIVOT (\n",
    "                CAST(AVG(delay) AS DECIMAL(4, 2)) AS AvgDelay, MAX(delay) AS MaxDelay\n",
    "                FOR month IN (1 JAN, 2 FEB)\n",
    "           )\n",
    "               ORDER BY destination\"\"\").show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
